%\section*{Current title of the thesis}
%Vorläufiger oder Arbeitstitel, max. 300 Zeichen
% Data-driven robotic grasp synthesis in a simulated environment.\\
% (Datengesteuerte Robotergreifenssynthese in einer Simulationsumgebung.) 

% \chapter{Introduction}
% \section{Motivation}
% \section{Problem Statement}
% \chapter{Related Work}
% \section{Grasp Synthesis Algorithms}
% \section{Reinforcement Learning in Grasping Algorithms}
% \chapter{Approach}
% \section{Reinforcement Learning Algorithms}
% \subsection{Markov Decision Process}
% \subsection{Actor-Critic}
% \subsection{Soft Q-learning???}
% \subsection{Soft Actor-Critic}
% \subsection{Proximal Policy Optimization}
% \section{Simulation Set-Up}
% \subsection{Simulation Engine}
% \subsection{Gym}
% \subsection{Stable baselines}
% \chapter{Implementation}
% \section{System Architecture}
% \chapter{Evaluation}
% \subsection{ende fertig-------------------}

\chapter{Introduction}
% \section{Motivation}
% Nowadays there is a big variety of use cases in the field of robotics in everyday life, starting from smart home devices to autonomous driving vehicles and space shift robots. Due to the significant development of soft- and hardware in the last decades it is becoming possible to create new robots that could considerably impact humans’ lives. \\ 
% In various scenarios robots have to grasp an object and then perform different actions on it: lift, shift, put on a specific position. It is crucial for the grasping part to succeed in order to be able to complete the proceeding steps of the manipulation. \\ 
% The parameters of the grasping problem vary depending on the use case scenario. In some cases there are several objects on the surface, one of them (a specific one or a random) has to be grasped - this is a cluttered environment scenario. In other cases, the object is singulated. Objects can be already known in advance with existing 3D meshes of them (i.e.in the industry scenario), familiar or never seen before. They can have a specific shape (only cubes) or have a random form, be rigid or non-rigid, transparent or not etc. They type of gripper plays a role (i.e. a two- or a five-finger gripper) and whether there is a camera or several cameras to control the process. \\ 
% The current state-of-the art approach for the grasping problem in the industry is based on knowing the exact positions of the robotic gripper and the object, what kind of the object that is, its size and shape. So the grasping motion is also already predefined. However, if the some characteristics of the objects slightly change, the robot might not be able to grasp it anymore. \\                                                    
% The goal solution to the grasping problem would consist of the robot being able to grasp any object. It means that the robot should be able to adapt to novel objects that it never saw in the training - generalization. Another important aspect is minimizing human involvement in the training process: is it possible for the robot to learn how to grasp without a need for a human to spend hours or even days teaching him how to do so? Moreover, it would be preferable for the learning time of the robot to be realistic - if the training process lasts for years, the problem might change during that time and the training result might be not applicable anymore. \\
% Data-driven approaches based on reinforcement-learning have shown promising results for solving the grasping problem. Self-supervised systems collect training data without any human help and learn from it. They are able to generalize to novel objects. Training the robot in simulation is helpful for fast collecting large quantities of data with little cost. 
% Combining these approaches and then transferring to the real world set-up might be an effective way to solve the grasping problem.

%ensure stability, "Computing task-oriented grasps is consequently crucial for a
%grasping strategyy. Finally, because of the variety of objects shapes and sizes,
%grasping novel objects is required."


%The grasping problem has been profoundly researched throughout the last decades. Many different approaches have been developed. Since the beginning of the 21century, Machine Learning has been used in more and more of them. ——> Übergang zum Reinforcement learning 404 not found ——
%Reinforcement learning, one of the types of Maschine Learning, has shown promising results in business strategy planning(???), continuous control problems(?) and creating artificial intelligence for computer games. Reinforcement learning approach is similar to how humans learn. One of the simple everyday tasks that every human learns is catching a ball. Either we watch somebody else doing it or try it out ourselves. We automatically in the course of split seconds estimate the size and mass of the ball, what our position should be in order to catch it. The result of catching - positive or negative - influences our behavior for the next time when we have to perform the same task. (????)\\ 
%Robotic control pipeline Aussage von Sergey Levine - nicht (ein)verstanden.
%End-to-end training ?


\section{Motivation}
Nowadays there is a big variety of use cases in the field of robotics in everyday life, starting from smart home devices to autonomous driving vehicles and space ship robots. Due to the significant development of software and hardware in the last decades it is becoming possible to create new robots that could considerably impact humans’ lives. \\ 
In high number of various scenarios robots have to grasp an object and then perform different actions on it: lift, shift, put on a desired position. The robot will not be able to complete its task unless the target object is grasped successfully. Therefore grasping is a crucial problem in robotics that requires a precise solution. \\
The current approach for the grasping problem in the industry is mostly based on knowing the exact positions of the robotic gripper and the object, what kind of the object that is, its size, shape and position. So the grasping motion is also already predefined. If the characteristics of the object or its position slightly change, the robot might not be able to grasp it anymore.  This provides a set of limitations for the objects that can be grasped and the setup. \\   
The goal solution of the grasping problem would be the one that does not include any limitations. The robot should be able to generalize to objects of any size and shape, regardless of their positions and surrounding environment. It should be able to adapt to new surroundings, light conditions and noise. The higher degree of autonomy if desired: decisions should be based on the perception of the current environment rather than according to a preprogrammed behavior. \\
Due to the development of Machine Learning in recent years it has become possible to create intelligent systems that show high success rates in coping with different tasks: image and speech recognition, self-driving vehicles, artificial intelligence for game playing. Machine learning techniques have been also used in various approaches that aim to solve the grasping problem. Reinforcement learning -one of the types of machine learning -, in combination with deep learning results in the deep reinforcement learning approach that can be effectively used for the grasping problem. Deep reinforcement learning is based on learning from experience and can help combine perception and control. It has been used to create self-supervised grasping systems that are trained end-to-end with minimal to none human involvement. These systems have shown high success rates and proved to be able to generalize to novel objects. \\
In order to train a system based on the reinforcement learning approach, the robot has to complete millions of trial-and-error steps before it finds a good policy. Training the robot in simulation is helpful for fast collecting large quantities of data with little cost. After verifying that the suggested algorithm is indeed successful at solving the defined problem in simulation, it can be applied to the real world setting. The agent pretrained in simulation can be used for the grasping problem in real world using either additional training or techniques that help to overcome the reality gap problem. 


% Another important aspect is minimizing human involvement in the training process: is it possible for the robot to learn how to grasp without a need for a human to spend hours or even days teaching him how to do so? Moreover, it would be preferable for the learning time of the robot to be realistic - if the training process lasts for years, the problem might change during that time and the training result might be not applicable anymore.
                                             


\section{Problem statement}
The focus of this thesis is to develop a method for vision-based robotic object grasping in simulation using reinforcement learning approach. The learning-based approach must be data-driven and self-supervised: there is no information about the target object and no human involvement in the training process. The robot must learn how to grasp by trial-and-error. Using only the RGB image of the workspace, it must decide which action to take. During training it will get binary reward from the environment whether the object was successfully grasped or not and based on this information it must develop a policy. \\
The robot consists of a 7-DOF robotic arm with the two-finger jaw gripper. The grasps are planar. The end-to-end method must combine perception(vision) with robotic control. The training and testing will be conducted in a simulated environment. \\

% The focus of the thesis is to develop a method that enables the robot to learn how to grasp an unknown object using images from RGB camera as observation while using a two-finger parallel gripper. The learning process should be autonomous and self-supervised, avoiding any human intervention. he idea is to achieve this with the help of deep reinforcement learning approach. The training and testing will be conducted in a simulated environment.

% The approach is based on the works of Zeng et al. \cite{zeng2018learning}, \cite{zeng2019tossingbot}. \\
%In case of successful performance of the developed algorithm in the simulation, the next step will be to prepare the transition to the real-world environment with the help of domain randomization. \\
%One of the advantages of such algorithm is the ability to generalize to new objects. Previous research shows that data-driven algorithms that are based on machine learning have successful rates of grasping objects that were never used in training. Another advantage is the fact that no human labeling will be required. This spares time and effort. \\
%Training the robot in real world is time consuming and expensive, simulation makes it possible to avoid these costs. However, there is a so-called "reality gap" problem, when the algorithm trained in simulation does not perform well in the real world scenario. One of the possible solutions to this problem is using domain randomization and/or fine-tuning with training in reality. The sim-to-real transfer is an additional optional part of the thesis, that will take place only if the algorithm performs well in simulation.\\

\chapter{Related work}
\section{Approaches for Solving the Grasping Problem}
Sahbani et al. \cite{sahbani2012overview} divided different grasp synthesis algorithms in analytical and empirical. Analytical approaches investigate the physics, kinematics and geometry of the object and the robot in order to determine contact points and gripper position\cite{nguyen1988constructing},  \cite{murray2017mathematical}. 
Shimoga et al. Schimoga  \cite{shimoga1996robot} defined a force-closure grasp as the one that guarantees that "the fingers are capable of resisting any arbitrary force and/or moment that may act on the object externally". Four independent properties that are crucial for a successful force-closure grasp were listed: dexterity, equilibrium, stability, dynamic behavior. Analytical approaches concentrate on developing algorithms that satisfy these properties.\\
\cite{ding2000computing} of Ding et al. is an example of the analytical approach. Having a multi-fingered gripper with n fingers, an object and the position of m of n fingers that do not form a form-closure grasp, the position of the remaining $(m-n)$ fingers have to be determined to satisfy the requirement of the form-closure grasp. There is a Coloumb friction between object and fingers. In order for fingers not to slip while executing the grasp, the finger force must have a certain direction (lie in a friction cone), which can be expressed as a set of non-linear constraints. Calculations consider the center of mass of the object, combination of grasping force and torque, center of the contact points. A sufficient and necessary condition for form-closure grasps was formulated. \\ 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{graphics/form_closure}
	\caption{Outtake from "Computing 3-D Optimal Form-Closure Grasps" \cite{ding2000computing} of Ding et al. The image shows the position of the friction cone at a grasping point on the object's surface. In order for fingers not to clip while executing the grasp, the finger force must lie in the friction cone.}
\end{figure}
As Bohg et al. \cite{bohg2013data} stated, analytical approaches usually require exact 3D models of the object, rely on the knowledge of the object's surface properties, its weight distribution and are not robust to noise. \\
In the same paper a detailed overview of the data-driven grasp synthesis approaches was made. Grasp synthesis is defined as "the problem of finding a grasp configuration that satisfies a set of criteria relevant for the grasping task". Data-driven or also called empirical approaches sample various grasp candidates and then rank them using different strategies. \\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{graphics/grasp_hypothesis}
	\caption{Classification of different aspects that influence the grasping problem according to \cite{bohg2013data} of Bohg et al. The most important aspect is the prior knowledge of the object.}
\end{figure}
Human demonstrations can be used to generate data for learning. In \cite{ekvall2007learning} magnetic trackers were placed on the hand of the human who was grasping and moving objects on the table. The robot recognized which object was moved and which grasp type was used, it then reproduced the task using this information. \\
Another strategy is to compare the candidate grasp to labeled examples. Redmon et al. \cite{redmon2015real} use the Cornell grasping dataset  \cite{Cornellgraspingdataset} to compare the sampled grasps to the "ground truth" grasps from the dataset. The rectangle metric is used for filtering good grasps. The metric includes two requirements: the grasp angle must be within 30$^\circ$ of the ground truth grasp and the Jaccard Index ($J(A, B) = \frac {|A \cap B|} {|A \cup B|} $) of the predicted grasp and the ground truth is greater than 25 \%. \\
Empirical approaches can use analytical metrics for ranking grasp candidates or labeling robust grasps. Mahler et al. \cite{mahler2017dex} introduced a synthetic dataset that includes 6.7 million point clouds with parallel-jaw grasps and analytic grasp quality metrics. Objects in the dataset come from the database containing 1500 3D object mesh models (129 of them come from KIT object database \cite{kasper2012kit}). For every object robust grasps covering the surface were sampled, using the antipodal grasp sampling approach developed in Dex-Net 1.0 \cite{mahler2016dex}). For stable poses of each object planar grasps (grasps perpendicular to the table surface) are chosen, as well as corresponding rendered point clouds (depth images). The introduced dataset was used to train a neural network, that was also introduced in \cite{mahler2017dex}. The network accepts a depth image of the scene and outputs the most robust grasp candidate.  \\
Human labeling of possible grasps for objects has some significant disadvantages. First of all, it is time consuming. Secondly, there exist a number of possible grasps for every object, it is hard and even impossible to tag every one of them. Pinto et al. \cite{pinto2016supersizing} also remarked the fact that human labeling is "biased by semantics": as an example they describe usual human labeling of handles of cups, because that is how a person would most likely to grasp a cup, although there are more possible configurations for successful grasp. \\
This reasoning led to development of self-supervised systems, where a robot learns from its own experience during the trial and error process. This approach is inspired by the way humans learn. Pinto et al. \cite{pinto2016supersizing} used a CNN for assessing planar grasp candidates. The grasp candidate is represented as (x,y) coordinates - the center of a chosen part of the image(patch) - and as an angle q - the rotation of the gripper around the z-axis. The CNN estimates a 18-dimensional likelihood vector. Each component of the vector represents the probability whether the grasp will be successful at the center of the patch with one of the 18 possible rotation angles of the gripper. The grasp with the highest probability of the success is executed. Depending on the result of the grasp, the error loss is back-propagated through the network. This way, the system does not rely rather on analytical metrics nor on human labeled examples - it learns from its own experience. \\
Following the classification of \cite{bohg2013data}, the objects that have to be grasped can be divided into three categories: known, familiar and unknown objects.\\
For known objects the data-driven approaches for finding a successful grasp often consist of estimating the pose of the object and then choosing the suitable grasp candidate from a precalculated grasp database, known as "experience database". In \cite{tremblay2018deep} of Tremblay et al. the deep neural network takes as input only one RGB-image of the scene with known household objects and outputs belief maps of 2D keypoints of these objects. After that a standard perspective-n-point (PnP) \cite{lepetit2009epnp} algorithm uses these belief maps to estimate the 6-DOF pose of each object. For grasping the robot moves its arm to a point above the object and then completes a top-down grasp.\\
Another category of objects to grasp is familiar objects. Objects can have similarities in various aspects(texture, shape, etc.). Familiar objects might be grasped in similar ways, the difficulty consists in detecting these similarities between objects and then applying grasping experience on them. Manuelli et al. \cite{manuelli2019kpam} researched category-level object manipulation with the help of using semantic 3D keypoints as object representation. The two object categories examined were shoes and cups. The goal of robotic manipulation was not only grasping, but also positioning the objects in a specific predefined way (i.e. place shoes on a shelf, hang cups on the hook by the handle). First the database of manually labelled keypoints on 3D reconstruction of the objects in different training scenes was created. Then a neural network was used to detect these keypoints from an RGB-D image of the scene. The detected keypoints together with the RGB-D image were used to calculate a suitable grasp using a similar to the baseline learning-based algorithm demonstrated in  \cite{zeng2018robotic}.\\
In case of unknown objects, the object was never seen by the robot beforehand and never used in training. Many approaches are based on approximating the shape of the object and then choosing the grasp for it \cite{bohg2011mind}. In recent years the approaches that have been most successful at solving this type of grasping problem are based on the trial-and-error learning-based approach \cite{pinto2016supersizing}, \cite{zeng2019tossingbot}, \cite{zeng2018robotic}. 


\section{Simulation}
In data-driven approaches training data is required to learn for a successful grasp. Levine et al. \cite{levine2018learning} used 14 robotic arms that sampled 800 000 grasp attempts. Pinto and al. \cite{pinto2016supersizing} used one robot manipulator to conduct 50 000 grasp attempts in course of 700 hours. However, it is expensive to collect such amount of data and it is very time consuming, this is where the arguments for learning in simulation come to a point. \\
Goldfeld et al. \cite{goldfeder2011data} created a grasp planner containing database of grasps for 3D models of different objects, that were generated using GraspIt! \cite{miller2004graspit} simulation engine. Kasper et al. \cite{kasper2012kit}  introduced the system for digitizing objects. In year 2010 the OpenGRASP  \cite{leon2010opengrasp}  simulation toolkit for grasping and dexterous manipulation was created. \\
James et al. \cite{james2019sim} developed an approach that used only a small amount of real-word training data in addition to simulation, which helped to reduce the real-world data by more than 99\%. Bousmalis et al. \cite{bousmalis2018using} implemented a grasping method that helps to significantly reduce the amount of additional real-world grasp samples. In their experiment 50 times less real-world samples were required to achieve the same level of performance compared to their previous system. \\
Another advantage of using simulation is the ability to pretrain the network. Redmon et al. \cite{redmon2015real} stated that pretraining large convolutional neural networks significantly decreases training time and also helps to avoid overfitting".
However there are some drawbacks to synthetic data. Most significant one is the reality gap: the network trained in a simulated environment shows much worse performance in real world. An effective way to trying to close the reality gap is to use domain randomization \cite{tobin2017domain}. Tremblay et al. \cite{tremblay2018deep} used photorealistic data in addition to domain randomization. It added variation and complexity to the training, which helped for the trained neural network to be able to successfully operate in the real world scenario without any additional fine-tuning.\\
Another technique to overcome the reality gap is domain adaptation \cite{patel2015visual}. Bousmalis et al. \cite{bousmalis2018using} used domain adaptation to extend the grasping system trained in simulation. 

%there are methods for learning either from human
%demonstrations, labeled examples or trial and error.

%Dex-Net 1.0 is the first version of Dexterity Network, a set of 3D object models with multiple labelled grasps for each of them and an algorithm for planning parallel-jaw grasps. Dex-Net 1.0 researches finding a robust parallel-jaw grasp for a given object, using analytic and statistic sampling methods to assessing the grasp candidates. 2017 the Dex-Net 2.0 was introduced. The synthetic dataset includes 6.7 point clouds with parallel-jaw grasps and analytic grasp quality metrics (darf man das so abschreiben? lieber zitieren?). Also a Grasp-Quality Convolutional Neural Network that classifies robust grasp poses from depth images of singulated objects considering sensor and control imprecision, trained on the data from Dex-Net 2.0, was presented. The Input for GQ CNN is depth image in form of the 3D point cloud with an annotated set of pairs of antipodal points that represent a possible grasp. The network, trained on the Dex-Net 2.0 set outputs the most robust grasp candidate. \\ 
%Database generation: The database contains 1500 3D object mesh models (129 of them come from KIT object database). For every object robust grasps covering the surface are sampled, using the antipodal grasp sampling approach developed in Dex-Net 1.0. For stable poses of each object planar grasps (grasps perpendicular to the table surface) are chosen, as well as corresponding rendered point clouds (depth images).\\ 
%For assessment of the performance of GQ-CNN four criteria were used: success rate, precision, robust grasp rate, planning time.  \\ 



%. To study grasp planning at scale, Goldfeder et al. [12], [13] developed the Columbia grasp database, a %dataset of 1,814 distinct models and over 200,000 force closure grasps generated using the GraspIt! %sampling-based grasp planner. Pokorny et al. [34] introduced Grasp Moduli Spaces, enabling joint grasp and %shape interpolation, and analyzed a set of 100 million sampled grasp configurations.

%\section*{Zeitplan}

% \subsection{State of the art}




\section{End-To-End Learning}
In robotics the classical control over the robot consists of several steps, each one of them usually represented by a separate module which are connected with each other and pass the data to each other. For example: after retrieving an observation of the scene using a calibrated camera another module detects objects in the image. The next module might create a physics model of the scene to plan an action that needs to be executed. The next step would calculate the positions of robot's joints and after that the actual action would be executed. If an error is made in one of these steps, it might become even more significant in the next steps resulting in incorrect output. End-to-end approach suggests making such connection between modules, that can be changed and adapted during the learning. In that way mistakes that would happen in one of the modules would be corrected in the following ones so that in the end the output would not be influenced by them. \\
Levine et al. \cite{levine2016end} discovered that training perception and control systems end-to-end shows better results and consistency than training each component separately. 

\section{Vision-based Learning}
Grasping novel objects is challenging as the robot never saw these objects in training and there are no 3D-models available. The task is even more complicated when there are multiple objects in a cluttered environment that partially cover each other. In the past several attempts were made to create systems that can locate good grasping points from images of the scene \cite{saxena2008robotic}. Nowadays most data-driven grasping approaches use Convolutional Neural Networks(CNNs) to detect and grasp objects. CNNs can effectively learn to extract features from an image that are essential for grasping. \\
A variety of types of CNNs exist, for example VGG \cite{sengupta2019going}, Residual neural networks \cite{he2016deep}, Densely connected neural networks \cite{huang2017densely}. \cite{zeng2019tossingbot} uses ResNet to extract spacial features of the image which are then inputted to another Res-Net. \cite{liang2019knowledge} and \cite{zeng2018learning} use a DenseNet. Custom architecture for the CNN that delivers best results for the task can be designed \cite{berscheid2019improving}. \\
Multiple researches were aimed at developing the grasping system where the robot must calculate a grasping movement based only on the RGB or RGB-D image of the working space \cite{zeng2019tossingbot}, \cite{zeng2018learning}, \cite{bousmalis2018using}. 
Levine et al. \cite{levine2018learning} developed a grasping approach that is based on collecting large-scale data using multiple real robots and no human involvement in the training process, and getting continuous feedback on visual features using only over-the-shoulder RGB camera. This end-to-end approach uses raw pixel images as input and directly outputs task-space gripper motion. The paper addresses the issue of the need of massive analysis and planning in robotic manipulation tasks. The suggested methods avoids it by providing the system with continuous feedback from the setup, fragmenting the grasping attempt in several timesteps, getting an RGB-image and letting it decide what action to take at each timestep. This way the robot can correct its mistakes using previous experience, at the same time not requiring exact camera calibration. \\





% The approach consists of two major parts: the prediction network $g(T_t, v_t)$ that accepts visual input $I_t$ and a task-space motion command $v_t$ and outputs the probability of success of grasping after completing $v_t$. The second part is the servoing function $f(I_t)$ that uses the output of the prediction network to control the gripper for executing a good grasp. The grasping attempt consists of $T$ steps: The robot makes T steps before closing the gripper. It creates $T$ training samples: $(I^i_t, p^i_T - p^i_t, l^i)$: image, collected every step, vector that is calculated through reached pose at the end and the pose at the timestep $t$, and label $l_i$ that is the same for the whole attempt i and which denotes the success of the attempted grasp. ????? The whole self-supervised method can be interpreted as a type of reinforcement learning but without standard reinforcement learning algorithms.  \\

\section{Deep reinforcement learning for grasping problems}
One of the most important goals of learning-based approaches is for system to be able to perform effectively on previously unseen objects - generalization. Another essential aspect that is considered during development of the approach is the intention of minimizing human's participation in the training of the system. In recent years several approaches based on reinforcement learning have been successfully applied for solving the grasping problem \cite{pinto2016supersizing}, \cite{zeng2018learning},  \cite{berscheid2019improving}. With the help of reinforcement learning it is possible to create a self-supervised system that can learn through trial-and-error. This way the human involvement can be minimized as the robot collects training data by itself. Systems trained with reinforcement learning are also able to generalize to novel objects and scenarios.\\
Kaelbling et al. \cite{kaelbling1996reinforcement} made following definition for the reinforcement learning problem: "Reinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment." The trial-and-error interactions are the factor that is crucial for a self-supervised system: it supervises itself by committing an action, getting the result of this action and learning from it. \\
Quillen et al. \cite{quillen2018deep} compared different reinforcement learning off-policy algorithms for vision-based robotic grasping. The authors state that on-policy algorithms struggle with diverse grasping scenarios as the robot has to go back to previously seen objects in order to avoid forgetting the gained experience. Therefore off-policy methods might be preferred for the grasping problems. The criteria for evaluating the RL were: overall performance, data-efficiency, robustness to off-policy data and hyperparameter sensitivity - these factors are important when applying the grasping algorithms to robotic systems in real life. \\
Boularias et al. \cite{boularias2015learning} used reinforcement learning approach to grasp objects in dense clutter. The task was formulated as a Markov Decision Process (MDP). The state is represented as the RGB-D image of the scene. Action space consists of two types of actions: pushing and grasping, each action is an according vector. In a cluttered environment with a variety of objects sometimes the position of an object makes it hard to grasp it. Executing a pushing action on this or another object might help to gain better access to the object for grasping. The reward is 1 if the robot managed to successfully grasp an object, otherwise 0. The RGB-D image is primarily segmented into objects using spectral clustering \cite{von2007tutorial}. \\
% The state-of-the art end-to-end self-supervised system for solving the grasping problem is presented in \cite{zeng2018learning}. Zeng et al. used model-free deep reinforcement learning approach to modulate the system. In the MDP the RGB-D the state was represented as a RGB-D image of the scene. The actions are either a grasping or a pushing motion. The reward is 1 for successful grasps, 0.5 for pushing motions that make a considerable change to the environment, otherwise 0. The system consists of two DenseNet-121 neral networks (TODO cite) that take the image as input and output the probability that .  
Several of recent researches have achieved impressive results at solving the grasping problem with the extended version of Deep Q-Learning based on Convolutional Neural Networks. In \cite{zeng2018learning}, \cite{zeng2019tossingbot} and  \cite{liang2019knowledge} in the grasp planning part the robot estimates the state of the environment through RGB-D image of the workspace, it is used as input to a CNN. The workspace is represented as a discrete action grid with the same resolution as the input RGB-D image. The CNN is used as a Q-function approximator. The network outputs a probability map of the same size as the input image, each value (x, y) in the probability map is an estimated Q-value of completing a top-down planar grasp in the middle of pixel (x, y) which corresponds to grasping in the middle of the cell (x, y) of the action grid. The action is defined as the number of the cell (x, y) in the action grid and the rotation angle. The number of possible rotation angles is predefined: in \cite{liang2019knowledge} there are three possible rotation angles: 0$^\circ$, 45$^\circ$ and 90$^\circ$. \cite{zeng2019tossingbot} and \cite{zeng2018learning} used 16 possible rotations (different multiples of 22.5$^\circ$). Before inputting the image to the network, it is rotated by an according angle, altogether resulting in $n$ input images for one state, with $n$ -  the number of possible rotation angles. For each input image the probability map is calculated. Then the cell of the map with the biggest value across all output maps is greedily chosen and an according top-down grasp with the gripper rotated as the input image containing the biggest Q-value is executed. \\
This pixel-wise parameterization of state and action provides several advantages: the actions have a spatial locality to each-other, CNNs are efficient for pixel-wise computations, the training can converge with less data. \\
 It is necessary to point out that in these researches \cite{zeng2018learning}, \cite{zeng2019tossingbot}, \cite{liang2019knowledge} grasping was studied in combination with other actions: pushing in \cite{zeng2018learning}, sliding to wall in \cite{liang2019knowledge} and throwing in \cite{zeng2019tossingbot}. The approaches were trained end-to-end, which helped to reach high success rates. The reward function in \cite{liang2019knowledge} is binary: if the object is grasped 1, otherwise 0. In \cite{zeng2018learning} additionally the pushing action that has a noticeable change to the system is rewarded with 0.5. In \cite{zeng2019tossingbot}, the grasping part was trained with the help of obtaining a success label. Binary signal whether the throwing part succeeded was more efficient than calculating the antipodal distance between gripper fingers directly after the grasping part.\\
 RBG-D image as state representation, action space corresponding to image resolution, binary rewards, extending Deep Q-Learning based on CNN and end-to-end training is an effective way to train the robot to reach a high success rate in grasping and to quickly generalize to new objects and scenarios.



\section{Reinforcement learning}
Reinforcement learning is one of the types of Machine Learning. Combined with deep learning, it is a powerful technique for solving complex problems. Deep reinforcement learning has been used in different areas, such as games  \cite{vinyals2017starcraft}, \cite{mnih2013playing} and robotics \cite{kormushev2013reinforcement}, \cite{zeng2018learning}.\\

The core idea of reinforcement learning is having an agent that can perform actions in a specified environment. The agent gets observations from  the environment as an input, the output is an action. At all times the agent is in one of the predefined states, the actions that are possible in the current state are a subset of all actions set. For every performed action the agent receives a feedback in form of a reward from the environment. According to the reward the agent can decide whether the chosen action was the right decision. The core idea of reinforcement learning is modeled as Markov decision process (MDP), which consists of 4-tuple (S, A, R, P) \cite{kaelbling1996reinforcement}:
\begin{itemize}
	\item set of states S
	\item set of actions A
	\item a reward function $r_t = R(s_t, a_t, s_{t+1}), R: S \times A \times S \rightarrow \mathbb{R}$
	\item state transition function $P: S \times A ->P(S)$, where P(S) is a probability distribution over the set S (i.e. it maps states to probabilities)
\end{itemize}
In some notations the starting state distribution $\rho_0$  is defined as the fifth element of the MDP.
The name of the process comes from the concept of Markov chains: having a sequence of events the probability of each event depends only on the state that was achieved in the previous event, ignoring all events before. Markov state: $P(s_{t+1}|s_t) = P(s_{t+1}|s_1, ..., s_t)$.\\
The policy $\pi$ determines which action must be taken in each state \cite{spinningup}. The action might be deterministic: 
\begin{equation} 
a_t = \mu(s_t)
\end{equation}
or stochastic: 
\begin{equation} 
a_t \sim \pi(\cdot|s_t)
\end{equation}
Same for the state transition function: deterministic $s_{t+1} = f(s_t, a_t)$ or stochastic $s_{t+1} \sim P(\cdot | s_t, a_t)$. \\
Often there are multiple possible ways for the agent to accomplish a task. For example, there might be a short and a long path to a destination point, both of them conforming the requirements. However, the short path is better because it takes less time and less actions, so the agent should choose this path. This can be expressed as following: goal of the optimal policy is to maximize the sum of rewards during action sequence that leads to completing the task. There are multiple ways to define the sum of the rewards: 
\begin{itemize}
	\item  finite-horizon undiscounted return: R(t) = $\sum_{t=0}^{T} r(t) $, a straightforward sum of all rewards for all taken actions.
	\item  infinite-horizon discounted return: R(t) = $\sum_{t=0}^{\infty} y^{t}r(t) $, where y is a discount factor $\in$ (0,1). The discount factor makes sure the actions that are taken soon are more relevant that the ones that are taken many steps later. From mathematical point of view, the discount factor is one of the conditions that the infinite sum converges to a finite value.
\end{itemize}
Value function of the state $s$ is an expected return that the agent will get if it starts in state $s$ and acts according to the policy. Action-value function adds dependency to an action $a$:  expected return after starting in state $s$ and taking action $a$, continuing by acting forever according to the policy $\pi$: 
\begin{equation} 
Q^{\pi}(s,a) =\underset{\tau \sim \pi} {E}[{R(\tau)\left| s_0 = s, a_0 = a\right.}]
\end{equation}
The optimal action-value function Q*(s,a) gives the expected return if the agent starts in state $s$, takes an arbitrary action $a$, and then forever after acts according to the optimal policy in the environment: \\
% Finally, the Optimal Action-Value Function, Q*(s,a):
\begin{equation} 
Q^*(s,a) = \underset {\pi} {\max}  \underset {\tau \sim \pi}{E}[{R(\tau)\left| s_0 = s, a_0 = a\right]}
\end{equation}
where policy $\pi$ is optimal. \\
There is a connection between value and action-value functions, that is helpful for some calculations: 
\begin{equation} 
V^{\pi}(s) = \underset {a\sim \pi} {E} [{Q^{\pi}(s,a)}]
\end{equation}
 - the value function of the state is an expected return of the Q-function in this state if the action a taken comes from the policy $\pi$. \\
and
\begin{equation} 
V^*(s) =  \underset {a} \max {Q^* (s,a)}
\end{equation}


\section{Reinforcement Learning Algorithms}

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=1.0\textwidth]{graphics/taxonomy_of_RL_algorithms}
%	\caption{Reinforcement learning algorithms taxonomy Quelle: Spinning up
%		Ich werde ein ähnliches Bild machen, aber nur mit Algorithmen, die ich benutzt %habe.}\label{fig:RL}
%\end{figure}

There is a wide variety of reinforcement learning algorithms. Reinforcement learning algorithms can be categorized in model-based and model-free. As the name suggests, model-based algorithms have a model of the environment and use it to find an optimal policy. 
% There are some advantages and disadvantages of both approaches. Deisenroth et al. \cite{deisenroth2013survey} talk about "robot control as a reinforcement learning problem": forming the trajectory of the robot, which is sequence of states and motor commands that lead to them. Model-free policy search methods usually use real robots to sample such trajectories, which in most cases requires human supervision and causes fast wear-and-tear of robots, especially the ones that are not industrial. What is more, it is time consuming. Model-based methods aim to develop efficiency by sampling some observation trajectories and building a model out of them. The model should decrease the number of real-robot manipulations and better adapt to new unseen environments or parameters. However, in practice, such models can be used not exact enough, which leads to learning a poor policy. \\
Bansal et al. \cite{bansal2017mbmf} state that model-free reinforcement learning approaches are effective at finding complex policies, however they sometimes take very long time to converge. Model-based algorithms might be better at generalizing and reduce the number of steps to find an optimal policy, however without exact model the learned policy might be far from an optimal one. Also the model must be updated together with the policy.\\
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=1.0\textwidth]{graphics/model-basedRL.png}
%	\caption{Model-based RL Quelle: s}\label{fig:RL}
% \end{figure}

Model-free reinforcement learning algorithms can also be divided in being on- and off-policy. Policy is used in reinforcement learning to decide which action to perform in the current state. While learning and building the policy up, the algorithm does not necessarily need to always chose the action that the latest version of the policy suggests. The chosen action might be for example the one that will maximize the value function for the state as in Q-learning. In that case the algorithm is off-policy. In the opposite case, when algorithm strictly follows the policy while learning, it is an on-policy one.

\begin{comment}
\subsection{Q-Learning}
Q-learning is a model-free algorithm that is based on approximating an objective function in order to find the optimal policy. \\
$Q^*(s,a) =R(s,a) + \gamma \underset {s' \subset S} { \sum} P(s_{t+1}| a_t, s_t) * \underset {a'} {\max Q*(s', a')}$\\
R(s,a) is the immediate reward in state s for executing action a. \\
Since  $V^*(s) =\underset {a} {\max} Q^*(s, a)$, the optimal policy can be calculated as: \\
$arg\underset {a} {\max} Q^*(s, a)$ \\
The strategy for choosing the action in the current state is $\varepsilon$-greedy: with the probability $\varepsilon$ the chosen action will be calculated through the Q-function: a = $arg\underset {a} {\max} Q^*(s, a)$. With probability (1 - $\varepsilon$) the sampled action will be a random one from the action space: $a \in A(s)$. \\
The O-learning rule is: \\
$Q(s,a) =Q(s,a) + \alpha (r + \gamma \underset {a'}{\max} Q(s+1, a') - Q(s,a))$, \\
where  $\alpha$ is the learning rate and $Q(s,a)$ is the old value. \\
Usually all state-action pairs are stored in a table, a so-called q-table. The agent refers to this table to select the best action - the action with the biggest q-value - for the state he is in. \\
An agent is exploiting if he uses the q-table and selects the action with the biggest Q-value to perform next. An agent is exploring, if he ignores the table values and takes a random action. Exploring is important as the agent finds other states with possibly better results, that would not be discovered if the agent strictly followed the table. Exploitation/exploration can be controlled by an $\varepsilon$-value - how often should the agent perform a random exploration step. \\
Reinforcement learning is often used for complex problems with a wide action and state space, which makes it impossible to store all Q-values in a table because of the a big amount of time for calculation of the values of the table and the amount of memory that would be required to save the table.  Deep Q-Learning (DQN) is the variant of the Q-Learning which is one of the possibilities to deal with this problem with the help of a neural network as a function approximator. \\
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{graphics/DQN}
	\caption{TODO: make my own scheme}\label{fig:RL}
\end{figure}
The Q-values for all possible actions of the state are calculated, the one that maximizes the Q-Value is chosen as the next action. \\ 
The updating rule is analogic to the the standard Q-learning approach: \\
$T = R(s,a) + \gamma \underset {a'}{\max}Q_\theta(s',a') $ - target value. \\
$\theta = \theta + \gamma \nabla_\theta E_{s' \sim  P(s'|s,a)}[(T - Q_\theta(s,a))^2]$\\
$T - Q_\theta(s,a))^2$ is the temporal difference in form of the mean square error. The gradient of the error is used for calculating new weights for the network. \\
While the error is calculated only for one state, the gradient of the error impacts all weights in the network - all states. This makes the learning become unstable. In order to cope with this problem another network with the same architecture - target network - is used to compute the target value T. Target network is initialized with the same weights as the main function approximator network, and it is updated every defined number of steps. \\
Replay buffer is a technique that is also used in DQN. The agent's transitions $(s_t, a_t, r_t, s_{t+1})$ are saved to a replay memory D. After collecting some number of transitions, mini-batches containing random transitions from the replay buffer are sampled and then used for training the network with stochastic gradient descent(SGD). Due to the randomness of trajectories in mini-batches the learned knowledge does not tend to resemble only one type of trajectories. It is also more data-efficient as the experience data can be used multiple times for learning. \\
Replay buffer and target network make Q-learning stable and efficient. Q-learning is one of the most popular reinforcement learning methods as it is simple to implement. However, it has its disadvantages. \cite{hasselt2010double}  stated that because of the fact that it uses max operator to calculate the Q-value for the state, there are often significant overestimations of these values. 
% Double Q-Learning is an off-policy value based reinforcement learning algorithm that uses two different Q-functions: one for selecting the next action and the other for value evaluation. 
\end{comment}


%\begin{align*}
%V^{\pi}(s) &= \underE{a \sim \pi \\ s'\sim P}{r(s,a) + \gamma V^{\pi}(s')}, \\
%Q^{\pi}(s,a) &= \underE{s'\sim P}{r(s,a) + \gamma \underE{a'\sim \pi}{Q^{\pi}(s',a')}},
% \end{align*}


% \subsection{Deep Deterministic Policy Gradient (DDPG) }
%Deep Deterministic Policy Gradient (DDPG) is an off-policy algorithm similar to Q-Learning - it learns an optimal action-value function $Q^*(s,a)$. The main difference between these two algorithms is that DDPG works with continuous action spaces and Q-Learning with discrete ones. For discrete action spaces there are finite number of actions, so Q-values for all possible actions can be calculated and compared, the biggest one would be the max value. However this approach would not work in continuous action spaces as there are endless number of possible actions.  .....TODO
%uses a deterministic target policy


\begin{comment}
\subsection{Actor Critic}
The "Critic" estimates the value function (Q function = action-value function or V function = state-value)
"Actor" "updates the policy distribution in the direction suggested by the Critic (such as with policy gradients)."\\
TODO
Value-based methods are based on maximizing a Q-function that is usually given by Bellmann equation(TODO), the optimal policy can be then calculated as $\pi(s)=arg\underset {a} {\max} Q^*(s, a)$. Policy-based methods learn the policy directly without the value-function. The goal for policy-based approches is to maximize the cumulative reward of the trajectory(TODO Definition) $J(\theta) = E_{\tau \sim \pi_{\theta}[R_\tau]}$ as opposed to the value-based approach where the goal is to minimize the error function (which is normally in the form of the temporal difference error). Policy gradient searches for parameters that maximize the goal function by moving in the direction of the gradient - gradient accent: $\theta_{k+1} = \theta_k + \alpha\nabla_{\theta}J(\theta)$, where $\nabla_{\theta}J(\theta) = E_{\tau \sim \pi_{\theta}}[\sum_{t=0}^T\nabla_\theta log\pi_\theta(a_t|s_t)R(\tau)]$, $R(\tau)$ is the cumulative reward of the trajectory. When the reward of the sampled trajectory is positive, the gradient of the goal function is also positive, the policy will be optimized in the direction of the gradient, this way it learns that the sampled trajectory was a good one and vice versa.  Discrete stochastic policy gives as output success probabilities for all actions (Karams Folie). Continuous stochastic policy-based approach makes it possible to work with continuous action spaces. (stochastic policy gives probability distribution over all actions)\\
The idea of the actor-critic method is to use a Q-function instead of the (average??) cumulative reward $R(\tau)$ - the Q-function of the state gives an expected future reward after completing action a. The approximated policy gradient will then be:\\
$\nabla_{\theta}J(\theta) = E_{\tau \sim \pi_{\theta}}[\sum_{t=0}^T\nabla_\theta log\pi_\theta(a_t|s_t)Q_w(s,a)]$ \\
where w are the weights of the neural network that calculates the Q-function. That is a separate network that is called "the critic". The network that calculates the goal function is the "actor". The critic estimates the value-function, the actor uses it to update its policy.
TOTO actor-crtitic scheme from Sutton Barto find it it's good!!
\end{comment}


\subsection{Soft Actor-Critic}
Soft Actor-Critic(SAC) is a off-policy state-of-the art reinforcement learning algorithm  \cite{haarnoja2018soft}. As the name suggests, it is based on the actor-critic approach. The key idea of SAC is the actor trying to maximize not only the expected return but also the entropy. The algorithm is stable and also sample-efficient, it is capable of solving high-dimensional complex tasks. \\
Entropy is a measure of randomness in the policy. The term 'entropy' comes from the area of information theory and means the amount of information or 'surprise' of the possible outcome of the variable. When the outcome of some source of data is rather unexpected because it has less probability, it's entropy is high. \\
Entropy H of x from its distribution P: 
\begin{equation} 
H(P) = \underset {x \sim P} E [-\log P(x)]
\end{equation}
Increasing entropy results in more exploration, which can accelerate learning later on. It can also prevent the policy from prematurely converging to a bad local optimum. \\
At each timestep the agent gets additional to the immediate reward $R(s_t, a_t, s_{t+1})$ a bonus reward proportional to entropy:  $\alpha H(\pi(\cdot |s_t)$.

% Usually the main goal of reinforcement learning approaches is to maximize the expected % sum of rewards: \\
% $\sum_t E_({s_t, a_t)\sim\ \rho_\pi} [r(s_t, a_t)]$ \\

In SAC the goal of the algorithm is to find a policy that will maximize the objection: 
\begin{equation} 
J(\pi) = \underset {\tau \sim \pi } E [R(\tau)] = \underset {\tau \sim \pi } E [\sum_{t=0}^\infty 
\gamma^t( R(s_t, a_t, s_{t+1})+ \alpha H(\pi(\cdot |s_t))) ]
\end{equation}
The target policy is: 
\begin{equation} 
\pi^* = arg \underset \pi max J(\pi) = arg \underset \pi max \underset {\tau \sim \pi } E [\sum_{t=0}^\infty 
\gamma^t( R(s_t, a_t, s_{t+1})+ \alpha H(\pi(\cdot |s_t))) ]
\end{equation}

$\alpha$ is the temperature parameter that controls the influence of the entropy term on the reward function.

% Wikipedia:\\
% Given a random variable X, with possible outcomes $x_i$, each with probability $P_X(x_i)$, % the entropy H(X) of X: \\
% $H(x) = - \sum_i P_X(x_i) log_b P_X(x_i) = \sum_i P_X(x_i) I_X (x_i) = E[I_X]$ \\

The implementation of SAC includes five neural networks which are aimed to enable learning of three functions:
Parameterized state value function $V_\psi(s_t)$ , soft Q-function $Q_\theta(s_t, a_t)$, and a tractable policy $\pi_\phi(a_t|s_t)$. The parameters of these networks are $\psi$, $\theta$, and $\phi$. Additionally there is a target value network $V_{\bar\psi}$. The algorithm uses two Q-functions $\theta_1$ and $\theta_2$ which helps to avoid overestimations and speed up the training.\\
First the agent collects some experience which is saved in the replay buffer D (distribution of previously sampled states and actions). After that all five networks are updated based on the information from the replay buffer.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\textwidth]{graphics/SAC}
	\caption{Description of the SAC algorithm from the original paper. First the agent collects some experience which is saved in the replay buffer D. After that all five networks are updated using information from the replay buffer.}
\end{figure} 

\begin{comment}
Value network: \\
$V(s_t) = E_{a_t \sim \pi} [Q(s_t, a_t) - log \pi (a_t|s_t)] $ \\

The soft value function is trained to minimize the squared residual error: \\
$J_V(\psi) = E_{s_t \sim D} [\frac 1 2 (V_\psi(s_t) - E_{a_t \sim \pi_\phi} [Q_\theta(s_t, a_t) - log \pi_\phi (a_t | s_t)])^2]$

+ Target value network $V_{\bar\psi}$\\ 

The soft Q-function parameters can be trained to minimize the soft Bellman residual: \\
$Q(s_t, a_t) = r(s_t, a_t) + \gamma E_{s_{t+1}} [V_\psi(s_{t+1})] - targetQ value$ \\
$J_Q(\theta) = E_{(s_t, a_t) \sim D} [\frac 1 2 (Q_\theta(s_t, a_t) - \hat{Q} (s_t, a_t))^2]$ \\
$\hat{Q} (s_t, a_t) = r(s_t, a_t) + \gamma E_{s_{t+1} \sim p} [V_{\hat{\psi}} (s_{t+1})]$

The Q-function is learned on-policy => Value function as well. \\
Two Q-functions with $\theta_i$ are trained independently to optimise $J_Q(\theta_i)$. The minimum of two functions is used in computing the value gradient $\hat{\nabla}_\psi J_V(\psi)$ and policy gradient $\hat{\nabla}_\phi J_\pi(\phi)$. \\

Quote paper: The policy parameters can be learned by directly minimizing the expected
KL-divergence.
$\pi_{new} = arg \underset {\pi' \in \Pi} {min} D_{KL} (\pi'(\cdot | s_t) || \frac {exp(Q^{\pi_old} (s_t, \cdot))} {Z^{\pi_old (s_t)}})$ \\

$J_\pi(\phi) = E_{s_t \sim D} [D_{KL} (\pi(\cdot | s_t) || \frac {exp(Q_\theta (s_t, \cdot))} {Z_\theta(s_t)})]$ \\

Reparameterizatin trick: \\
Normally actions are sampled $a_t \sim \pi_\phi$ \\
the policy is reparameterised using network transformation: \\
$a_t = f_\phi(\epsilon_t; s_t)$ \\
for example $a_t = \mu_\phi(s_t) + \epsilon_t \sigma_\phi(s_t)$ where $\epsilon_t \sim N(0, 1)$, $\mu_\phi(s_t)$ is the mean action, $\sigma_\phi(s_t)$ variance. So instead of sampling $a_t \sim \pi_\phi(s_t)$ we can sample  $\epsilon_t \sim N(0, 1)$. Then: \\
$Q_\theta(s_t, a_t) = Q_\theta(s_t, \mu_\phi(s_t) + \epsilon_t \sigma_\phi(s_t))$  -smaller variance. \\ The network computes $\mu_\phi(s_t)$ and $\sigma_\phi(s_t)$

Due to the reparameterizatin trick: \\
$J_\pi(\phi) = E_{s_t \sim D, \epsilon_t \sim N} [log\pi_\phi (f_\phi (\epsilon_t; s_t) | s_t) - Q_\theta(s_t, f_\phi(\epsilon_t; s_t))]$ where $\epsilon_t$ is the input noise vector , sampled from some fixed distribution, such as a spherical Gaussian.

polyak-averaging???
\end{comment}

\subsection{Proximal Policy Optimization Algorithm}
Proximal Policy Optimization (PPO) \cite{schulman2017proximal} is the state-of-the-art on-policy algorithm. The main idea of the algorithm is following: after computing the update, the new policy should be not to far away from the old one. This idea is shared with another on-policy algorithm TRPO \cite{schulman2015trust}, however PPO algorithm is easier to implement and in has shown better performance on multiple reinforcement learning problems \cite{schulman2017proximal}. \\
The policy update rule of the PPO algorithm:
\begin{equation} 
\theta_{k + 1}:=  arg \underset \theta max \underset {s, a \sim \pi_{\theta_k}} E \left[ L(s, a, \theta_k, \theta) \right ]
\end{equation}
where $\theta$ is the policy parameter, 
\begin{equation} 
L(s, a, \theta_k, \theta) = min (\frac {\pi _\theta(a|s)} {\pi _{\theta_k}(a|s)} A^{\pi_{\theta_k}} (s,a), g(\epsilon, A^{\pi_{\theta_k}} (s,a)))
\end{equation}
$\frac {\pi _\theta(a|s)} {\pi _{\theta_k}(a|s)}$ is the ratio of the probability under the new and old policies, $A^\pi$ - advantage function, $\epsilon$ is a hyperparameter which sets a limit how far can an new policy be far away from the old one. The advantage function $A^\pi(s, a)$ helps to determine how much better the action $a$ in state $s$ is in comparison to the random selected action according to $\pi(\cdot|s) : A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$.
\begin{equation} 
g(\epsilon, A) = \begin{cases} 
(1 + \epsilon)A & A \geq 0 \\
(1 - \epsilon)A & A < 0 \\
\end{cases}
\end{equation}


\chapter{Approach}
In order to combine vision with robotic grasping, the Deep Reinforcement Learning approach is used. The goal is to develop a policy according to which the agent will decide what action he will take at the current state. The policy is trained through trial-and-error: after getting information about the current state, the agent takes an action. The environment yields reward for this action and the next state, that the action lead to. With the help of this information the agent builds up its policy.
\begin{itemize}
	\item State representation: \\
	The state is represented as an RGB-image of the work surface where the object is located. In the image part of the surface and the object can be seen, the image does not include the gripper. 
	\item Actions \\
	The robot movements are modeled as planar grasps. In the beginning of the iteration the gripper has height z over the table. The actions are target (x, y) coordinates: the gripper goes to (x,y) preserving its height z.  Afterwards the planar grasp takes place: the gripper goes down, changing only its z-coordinate and having maximal jaw opening. Then the gripper closes and goes back up. \\
	In some experiments the gripper yaw-rotation is part of the action space: (x, y, $\alpha$). After reaching target (x, y) position, the gripper rotates the angle $\alpha$ and then completes a top-down planar grasp. 
	\item Reward function: \\
	 The reward function is binary: it is 1 if the robot successfully lifts the object, otherwise 0. In order to register whether the grasp was successful, the distance between the gripper jaws is compared to a threshold value that corresponds to the width of the object. If the distance exceeds the threshold, the object is located between the jaws at the end of the manipulation, which means it was grasped successfully. 
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{graphics/Approach}
	\caption{The goal of the Deep Reinforcement learning approach is to develop a policy that decides which action at which step the agent should take. In order to do that the agent interacts with the environment by observing environment's state, taking actions and getting rewards for them. This way the agent determines through trial-and-error the correct behavior. Source of the image: \cite{mao2016resource}}
\end{figure}

At all times there is only one object on the table that is the same during the whole training. 

\chapter{Implementation}

\section{System components}
Mujoco \cite{todorov2012mujoco} was chosen as simulation environment as it has shown faster and more accurate performance than other simulators. As a base for the simulation the gym "FetchPickAndPlace-v0" environment \cite{pickplace2018openai} was used. Implementation of the reinforcement learning algorithms SAC and PPO were taken from the stable baseline \cite{stable-baselines} library.  


\subsection{Mujoco}
Mujoco (Multi-Joint dynamics with Contact) is a physics engine developed for model-based control \cite{todorov2012mujoco}. It was developed at "Movement control laboratory", University of Washington for research projects in the area of 'optimal control, state estimation and system identification", as none of already existing tools delivered a satisfying performance. Mujoco has shown more stable, fast and accurate performance for robotic tasks in comparison to other physics simulators \cite{erez2015simulation}. \\
Mujoco is user-convenient as well as computation efficient. The model can be specified in a an XML-file format. The visualization is interactive and done by the rendering library OpenGL \cite{opengl}. Some of the key features of Mujoco include:
\begin{itemize}
	\item Generalized coordinates combined with modern contact dynamics
	\item Soft, convex and analytically-invertible contact dynamics
	\item Separation of model and data
\end{itemize}
and many more. Further description of Mujoco and its documentation can be found on the official website \cite{mujoco-documentation}.


\subsection{OpenAI Gym}
Gym is toolkit developed by OpenAI \cite{openai} for researching in the area of reinforcement learning. Gym is an open-source library which includes collection of environments for different reinforcement learning problems. They include Atari (i.e. Breakout-v0) and Board games (Go), Robotics(HandManipulateBlock-v0) and many more \cite{1606.01540}. The user can modify the environment and construct its own agent. \\

\subsection{Stable Baselines}
Baselines \cite{baselines} is a library developed by OpenAI \cite{openai} containing implementations of different reinforcement learning algorithms. Stable baselines \cite{stable-baselines} is a collection of improved RL algorithms based on OpenAI baselines. The algorithms of stable baselines have unified structure, are better documented, there are more tests for them. Moreover, some new reinforcement learning algorithms were represented. They can be used in combination with gym environments. Stable baselines also include a set already pretrained agents \cite{rl-zoo}. \\
In order to train the agent using reinforcement learning algorithms from stable baselines the environment must follow the gym interface: it must implement methods {\_\_init\_\_()}, step(), reset(), render(), close() and inherit from gym.Env.
\begin{figure*}[h!]
	\centering
	\includegraphics[width=0.7\textwidth]{graphics/CustomEnv}
	\caption{Custom environment example for using stable baselines. The CustomEnv  inherits from gym.Env and implements methods {\_\_init\_\_()}, step(), reset(), render(), close() which is a requirement to be able to train using one of the reinforcement leaning algorithm's implementation from stable baselines.}
\end{figure*}\\


\section{Simulation Setup}
As a base for the simulation the gym "FetchPickAndPlace-v0" environment \cite{pickplace2018openai} was used: "Fetch has to pick up a box from a table using its gripper and move it to a desired goal above the table". \\
The action space in this environment, that consisted of changes of grippers coordinates and rotations, was adjusted to have only two values: target x and y coordinates, as well as the target rotation of the gripper in some experiments. As for the observation space, in "FetchPickAndPlace-v0" it is represented as a list of different values, such as positions, rotations, velocities of the gripper and the object. For the current environment it was changed to consist only of an RGB-image. The reward function was modified from the dense to a sparse one: in "FetchPickAndPlace-v0" environment it is represented as the distance between achieved and desired goal, whereas in the modified version it is binary - 1 if the object was grasped and 0 otherwise. \\
The step() function was adjusted in order to modulate the planar grasp: the gripper goes to (x,y) preserving its height z.  Afterwards the planar grasp takes place: the gripper goes down, changing only its z-coordinate and having maximal jaw opening. Then the gripper closes and goes back up. \\
Each training episode lasts at most 50 steps which is defined by the max\_episode\_steps parameter. It means the robot has 50 attempts to grasp the object before the algorithm switches to the next episode.

\section{Observation Space}
The camera is adjusted to take an RGB-image of the part of the table where the object can be located. The image represents the state of the environment. The 81x81 pixels with 3 channels for RGB result in a 81x81x3 input to the neural network. 
\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{graphics/camera_setup}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{graphics/observation_2.png}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}
	\caption{The camera on the side of the table takes an RGB-image of the part of the table where the object can be located. The second image is an example of the observation that is used as state representation and input to the neural network}
\end{figure} 


\section{Reward Function}
The agent is granted with the reward 1 if he successfully grasps the object, otherwise 0. In order to determine whether the grasp was successful, the distance between the gripper jaws is compared to a threshold value that corresponds to the width of the object. If the distance exceeds the threshold, the object is located between the jaws at the end of the manipulation, which means it was grasped successfully. 
\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{graphics/gripper_closed}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{graphics/gripper_opened}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}
	\caption{In the first image the gripper's jaws are fully closed after the grasped attempt - the object was not grasped. In the second image the attempt was successful, the object is between the jaws preventing them from closing, so the distance between the jaws is slightly greater or equals the width of the object.}
\end{figure} 

\section{Action Space}\label{section:action_space}
Three possible approaches are used to represent the action space. Apart from using the straightforward continuous action space, the idea of discretizing action space as in \cite{zeng2018learning}, \cite{zeng2019tossingbot} is tested. With the help of these multiple approaches can be determined whether the representation of the action space can influence performance of some reinforcement learning algorithms. 

\subsection{Continuous Action Space}\label{section:continuous_action_space}
The action (x, y) is target Cartesian coordinates of the gripper: x $\in$ [1.1, 1.45] and y $\in$ [0.55, 0.95]. Continuous spaces are represented through the Box class of the gym library: $self.action\_space = spaces.Box(np.array([1.1, 0.55]), np.array([1.45, 0.95]), dtype='float32')$.\\
Example of the action: [1.2, 0.7].
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{graphics/continuous_action_space}
	\caption{Continuous Action Space: the action (x, y) is target cartesian coordinates of the gripper.}
\end{figure}

\subsection{Discrete Action Space without Rotation}\label{section:multidiscrete_no_rotation}
For this approach the workspace is represented as 50x50 grid. The action (x, y) means going to the middle of the cell number (x, y) and completing a planar grasp. Discrete action space consists of several discrete values: x and y are integers, x $\in$ [0, 49] and y $\in$ [0, 49]. Discrete action spaces are represented through the MultiDiscrete class of the gym library: $self.action\_space = spaces.MultiDiscrete([49, 49])$. Before executing the action, it is translated to cartesian coordinates. \\
Example of the action: (x,y) = (2, 40) which corresponds to (1.1175, 0.9145).\\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{graphics/multidiscrete_no_rotation}
	\caption{Discrete Action Space. The workspace is represented as 50*50 grid, the action (x, y) means going to the middle of the cell number (x, y) and completing the planar grasp.}
\end{figure}

\subsection{Discrete Space with Rotation}\label{section:multidiscrete_with_rotation}
As in the \hyperref[section:multidiscrete_no_rotation]{previous} method, the workspace is  represented as 50x50 grid, action (x, y) means going to the middle of the cell number (x, y). In this method one more variable is added to the action space - the target yaw gripper angle $\alpha$. $\alpha$ is an integer, $\alpha$ $\in$ [0, 6]. Before completing the top-down planar grasp, $\alpha$ is translated to degrees and the gripper is rotated in a degrees value that corresponds to the $\alpha$ angle. \\
The angle range for the gripper is limited to [0$^\circ$, 90$^\circ$] with the 15$^\circ$ step, resulting in 7 possible rotations which are expressed by the $\alpha$ value.
Example of the action: (x,y, $\alpha$) = (2, 40, 3) which corresponds to (1.1175, 0.9145, 45$^\circ$). 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{graphics/multidiscrete_with_rotation}
	\caption{Discrete Action Space including rotation. The workspace is represented as 50*50 grid, the action (x, y, $\alpha$) means going to the middle of the cell number (x, y), rotating the gripper value in degrees that corresponds to $\alpha$ and completing the planar grasp.}
\end{figure}

\section{Learning Algorithms}
Two algorithms are tested for solving the task: Soft-Actor Critic and Proximal Policy Optimization. Both are leading reinforcement learning algorithms. \\
\subsection{The Soft-Actor Critic Algorithm}
Soft-Actor-Critic (SAC) is the state-of-the-art off-policy algorithm that can be used for environments with continuous action spaces, so it is used in combination with the \hyperref[section:continuous_action_space]{continuous} action space approach. 

If not explicitly stated otherwise, the hyperparameters that were applied during experiments are:
\begin{table}[H]
	\begin{center}
		\footnotesize
		\begin{tabular}{|l|l|l|l|l|l|}
			\hline
			\bf Hyperparameter & \bf Value  \\ \hline
			Discount factor $\gamma$ & 0.99\\ \hline
			learning rate & 0.0003 \\ \hline
			replay buffer size & 50000 \\ \hline
			learning starts (how many steps of the model \\ to collect transitions for before learning starts) & 100 \\ \hline
			train\_freq (update the model every train\_freq steps & 1 \\ \hline
			batch size & 64 \\ \hline
			$\tau$  (the soft update coefficient: “polyak update”) & 0.005 \\ \hline
			Entropy regularization coefficient & auto (learned automatically) \\ \hline
			target\_update\_interval (update the target network every \\ target\_network\_update\_freq steps)& 1 \\ \hline
			gradient\_steps (how many gradient update after each step) & 1 \\ \hline
			target\_entropy & auto \\ \hline
			action\_noise & None \\ \hline
			random\_exploration & 0.0 \\ \hline		
		\end{tabular}
	\end{center}
	\label{exampletab}
	\caption{Hyperparameters for for the applied SAC algorithm.}
\end{table}

\subsection{Proximal Policy Optimization Algorithm}
The Proximal Policy Optimization(PPO) is the state-of-the-art off-policy algorithm that can be used for environments with continuous and discrete action spaces. It was tested in combination all three \hyperref[section:multidiscrete_no_rotation]{approaches} for representing action space. \\
The hyperparameters that were applied during experiments are:
\begin{table}[H]
	\begin{center}
		\footnotesize
		\begin{tabular}{|l|l|l|l|l|l|}
			\hline
			\bf Hyperparameter & \bf Value  \\ \hline
			Discount factor $\gamma$ & 0.99\\ \hline
			timesteps\_per\_actorbatch (timesteps per actor per update) & 256 \\ \hline
			clip\_param (clipping parameter $\epsilon$) & 0.2 \\ \hline
			entcoeff (the entropy loss weight) & 0.01 \\ \hline
			optim\_epochs (the optimizer’s number of epochs) & 4 \\ \hline
			optim\_stepsize & 0.001 \\ \hline
			optim\_batchsize & 64 \\ \hline
			lam (advantage estimation) & 0.95 \\ \hline
			adam\_epsilon& 1e-05  \\ \hline
			schedule (type of scheduler for the learning rate update ) & linear \\ \hline	
		\end{tabular}
	\end{center}
	\label{exampletab}
	\caption{Hyperparameters for for the applied PPO algorithm.}
\end{table}

% max\textunderscore episode\textunderscore steps is a parameter that defines how many times the step() function can be called before the episode ends - how many times can robot attempt to grasp in the current episode. \\
\section{CNN structure}
The network consists of three constitutional layers, followed by a fully connected layer. The activation function is rectified linear unit (ReLU). The weights were randomly initialized through orthogonal initialization (as an orthogonal matrix).

\begin{table}[H]
	\begin{center}
		\footnotesize
		\begin{tabular}{|l|l|l|l|l|l|}
			\hline
			\bf Layer & \bf Number of filters & \bf Filter size & \bf Stride \\ \hline
			Conv1 & 32 & 8 & 4\\ \hline
			Conv2 & 64 & 4 & 2\\ \hline
			Conv3 & 64 & 3 & 1\\ \hline		
			Fc1 & \multicolumn{3}{c|}{512 hidden neurons} \\ \hline	
		\end{tabular}
	\end{center}
	\label{exampletab}
	\caption{Hyperparameters for for the applied PPO algorithm.}
\end{table}





