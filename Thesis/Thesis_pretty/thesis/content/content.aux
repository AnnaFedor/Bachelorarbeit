\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem statement}{2}{section.1.2}\protected@file@percent }
\citation{sahbani2012overview}
\citation{nguyen1988constructing}
\citation{murray2017mathematical}
\citation{shimoga1996robot}
\citation{ding2000computing}
\citation{ding2000computing}
\citation{ding2000computing}
\citation{bohg2013data}
\citation{bohg2013data}
\citation{bohg2013data}
\citation{ekvall2007learning}
\citation{redmon2015real}
\citation{Cornellgraspingdataset}
\citation{mahler2017dex}
\citation{kasper2012kit}
\citation{mahler2016dex}
\citation{mahler2017dex}
\citation{pinto2016supersizing}
\citation{pinto2016supersizing}
\citation{bohg2013data}
\citation{tremblay2018deep}
\citation{lepetit2009epnp}
\citation{manuelli2019kpam}
\citation{zeng2018robotic}
\citation{bohg2011mind}
\citation{pinto2016supersizing}
\citation{zeng2019tossingbot}
\citation{zeng2018robotic}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related work}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Grasp Synthesis Algorithms}{3}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Outtake from "Computing 3-D Optimal Form-Closure Grasps" \cite  {ding2000computing} of Ding et al.\relax }}{3}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Classification of different aspects that influence the problem of the grasping problem according to \cite  {bohg2013data} of Bohg et al.\relax }}{4}{figure.caption.4}\protected@file@percent }
\citation{levine2018learning}
\citation{pinto2016supersizing}
\citation{goldfeder2011data}
\citation{miller2004graspit}
\citation{kasper2012kit}
\citation{leon2010opengrasp}
\citation{james2019sim}
\citation{bousmalis2018using}
\citation{redmon2015real}
\citation{tremblay2018deep}
\citation{levine2016end}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Simulation}{6}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}End-To-End Learning}{6}{subsection.2.2.1}\protected@file@percent }
\citation{saxena2008robotic}
\citation{levine2018learning}
\citation{he2016deep}
\citation{huang2017densely}
\citation{zeng2019tossingbot}
\citation{liang2019knowledge}
\citation{zeng2018learning}
\citation{quillen2018deep}
\citation{pinto2016supersizing}
\citation{zeng2018learning}
\citation{berscheid2019improving}
\citation{kaelbling1996reinforcement}
\citation{pinto2016supersizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Vision-based Learning}{7}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Deep reinforcement learning for grasping problems}{7}{subsection.2.2.3}\protected@file@percent }
\citation{quillen2018deep}
\citation{boularias2015learning}
\citation{von2007tutorial}
\citation{zeng2018learning}
\citation{zeng2019tossingbot}
\citation{liang2019knowledge}
\citation{zeng2019tossingbot}
\citation{liang2019knowledge}
\citation{zeng2018learning}
\citation{liang2019knowledge}
\citation{zeng2019tossingbot}
\citation{zeng2018learning}
\citation{zeng2018learning}
\citation{zeng2019tossingbot}
\citation{liang2019knowledge}
\citation{zeng2018learning}
\citation{liang2019knowledge}
\citation{zeng2019tossingbot}
\citation{liang2019knowledge}
\citation{zeng2018learning}
\citation{zeng2019tossingbot}
\citation{kaelbling1996reinforcement}
\citation{deisenroth2013survey}
\citation{bansal2017mbmf}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Algorithms in reinforcement learning}{11}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Reinforcement learning algorithms taxonomy Quelle: Spinning up Ich werde ein \IeC {\"a}hnliches Bild machen, aber nur mit Algorithmen, die ich benutzt habe.\relax }}{11}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:RL}{{2.3}{11}{Reinforcement learning algorithms taxonomy Quelle: Spinning up Ich werde ein Ã¤hnliches Bild machen, aber nur mit Algorithmen, die ich benutzt habe.\relax }{figure.caption.6}{}}
\citation{hasselt2010double}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Model-based RL Quelle: s\relax }}{12}{figure.caption.7}\protected@file@percent }
\newlabel{fig:RL}{{2.4}{12}{Model-based RL Quelle: s\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Q-Learning}{12}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces TODO: make my own scheme\relax }}{13}{figure.caption.8}\protected@file@percent }
\newlabel{fig:RL}{{2.5}{13}{TODO: make my own scheme\relax }{figure.caption.8}{}}
\citation{haarnoja2018soft}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Actor Critic}{14}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Soft Actor-Critic}{14}{subsection.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Custom environment example for using stable baselines\relax }}{16}{figure.caption.9}\protected@file@percent }
\citation{schulman2017proximal}
\citation{schulman2015trust}
\citation{schulman2017proximal}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Proximal Policy Optimization Algorithm}{17}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Approach}{19}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The goal of the Deep Reinforcement learning approach is to develop a policy that decides which action at which step the agent should take. In order to do that the agent interacts with the environment by observing environment's state, taking actions and getting rewards for them. This way the agent determines through trial-and-error the correct behavior. \relax }}{20}{figure.caption.10}\protected@file@percent }
\citation{todorov2012mujoco}
\citation{pickplace2018openai}
\citation{stable-baselines}
\citation{todorov2012mujoco}
\citation{erez2015simulation}
\citation{opengl}
\citation{mujoco-documentation}
\citation{openai}
\citation{1606.01540}
\citation{baselines}
\citation{openai}
\citation{stable-baselines}
\citation{rl-zoo}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{21}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}System components}{21}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Mujoco}{21}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}OpenAI Gym}{21}{subsection.4.1.2}\protected@file@percent }
\citation{pickplace2018openai}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Stable Baselines}{22}{subsection.4.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Custom environment example for using stable baselines. The CustomEnv inherits from gym.Env and implements methods {\_\_init\_\_()}, step(), reset(), render(), close() which is a requirement to be able to train using one of the reinforcement leaning algorithm's implementation from stable baselines.\relax }}{22}{figure.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Simulation Setup}{22}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Observation Space}{23}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The camera on the side of the table takes an RGB-image of the part of the table where the object can be located. The second image is an example of the observation that is used as state representation and input to the neural network\relax }}{23}{figure.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Reward Function}{23}{section.4.4}\protected@file@percent }
\citation{zeng2018learning}
\citation{zeng2019tossingbot}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces In the first image the gripper's jaws are fully closed after the grasped attempt - the object was not grasped. In the second image the attempt was successful, the object is between the jaws preventing them from closing, so the distance between the jaws is slightly greater or equals the width of the object.\relax }}{24}{figure.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Action Space}{24}{section.4.5}\protected@file@percent }
\newlabel{section:action_space}{{4.5}{24}{Action Space}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Continuous Action Space}{24}{subsection.4.5.1}\protected@file@percent }
\newlabel{section:continuous_action_space}{{4.5.1}{24}{Continuous Action Space}{subsection.4.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Continuous Action Space: the action (x, y) is target cartesian coordinates of the gripper.\relax }}{24}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Multidiscrete Action Space without Rotation}{25}{subsection.4.5.2}\protected@file@percent }
\newlabel{section:multidiscrete_no_rotation}{{4.5.2}{25}{Multidiscrete Action Space without Rotation}{subsection.4.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Multidiscrete Action Space. The workspace is represented as 50*50 grid, the action (x, y) means going to the middle of the cell number (x, y) and completing the planar grasp.\relax }}{25}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Multidiscrete Space with Rotation}{25}{subsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Multidiscrete Action Space including rotation. The workspace is represented as 50*50 grid, the action (x, y, $\alpha $) means going to the middle of the cell number (x, y), rotating the gripper value in degrees that corresponds to $\alpha $ and completing the planar grasp.\relax }}{26}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Learning Algorithms}{26}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}The Soft-Actor Critic Algorithm}{26}{subsection.4.6.1}\protected@file@percent }
\newlabel{exampletab}{{\caption@xref {exampletab}{ on input line 514}}{27}{The Soft-Actor Critic Algorithm}{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Hyperparameters for for the applied SAC algorithm.\relax }}{27}{table.caption.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Proximal Policy Optimization Algorithm}{27}{subsection.4.6.2}\protected@file@percent }
\newlabel{exampletab}{{\caption@xref {exampletab}{ on input line 539}}{27}{Proximal Policy Optimization Algorithm}{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Hyperparameters for for the applied PPO algorithm.\relax }}{27}{table.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}CNN structure}{27}{section.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The characteristics of the network. The network consists of three convolutional layers, followed by one fully connected layer. After each layer the ReLU function is applied.\relax }}{28}{figure.caption.19}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Evaluation}{29}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Simulation Inaccuracies}{29}{subsection.5.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline The Angle Of The Gripper}{29}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Slipping Of The Object}{29}{section*.22}\protected@file@percent }
\@setckpt{content/content}{
\setcounter{page}{30}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{0}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{1}
\setcounter{table}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{caption@flags}{4}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{4}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{section@level}{3}
}
