\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Evaluation}{27}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\newlabel{chapter:evaluation}{{6}{27}{Evaluation}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Experiments with the Soft-Actor-Critic Algorithm}{27}{section.6.1}\protected@file@percent }
\newlabel{exampletab}{{\caption@xref {exampletab}{ on input line 35}}{28}{Experiments with the Soft-Actor-Critic Algorithm}{figure.caption.16}{}}
\newlabel{sub@exampletab}{{}{28}{Experiments with the Soft-Actor-Critic Algorithm}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Results of the evaluation of best agents for each test scenario. These agents show high success rate and low variance. However if the trainings for each experiment will be repeated, the resulting agents might be not as successful as the ones represented in the table.\relax }}{28}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}One Constant Positions Of The Object}{28}{subsection.6.1.1}\protected@file@percent }
\newlabel{evaluation:one_obj_SAC}{{6.1.1}{28}{One Constant Positions Of The Object}{subsection.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces An example of unstable behavior of the algorithm based on the reward value: after 14k training steps the agent learned correctly where the object is located. Then the performance dropped to 0 and did not recover after that.\relax }}{28}{figure.caption.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}20 Constant Positions Of The Object}{28}{subsection.6.1.2}\protected@file@percent }
\newlabel{evaluation:20_obj_SAC}{{6.1.2}{28}{20 Constant Positions Of The Object}{subsection.6.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces After about 45k training steps the success rate was almost always 1. The reason it dropped in some episodes might be due to inaccuracy in actions or some object positions occurred for the first time - the agent never saw them before which lead to bad performance, they were learned after that\relax }}{29}{figure.caption.18}\protected@file@percent }
\newlabel{fig:sub-first}{{\caption@xref {fig:sub-first}{ on input line 75}}{29}{20 Constant Positions Of The Object}{figure.caption.19}{}}
\newlabel{sub@fig:sub-first}{{}{29}{20 Constant Positions Of The Object}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Statistics of the training: object's position is one of 20 possible ones, learn to grasp the object in 150k steps. At the end of the training the entropy is becoming constant which means that the algorithm is sure which action to take. The entropy coefficient is going down as well because the agent does not need to do much exploration anymore. Policy loss is going to zero, as well as the losses from both Q-networks and the target value network, which means the weights of the network are adjusted in an optimal way to achieve success. \relax }}{29}{figure.caption.19}\protected@file@percent }
\citation{haarnoja2018soft}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The first image shows the statistics of the length of first 10k episodes of the training, the second one - of the last 10k. It is noticeable, that in the second case the majority of episodes were short because the agent has already learned most of positions of the objects. In case of inaccuracy of actions or incorrect assumptions about the location of the object the episode lasted longer - up to 50 steps\relax }}{30}{figure.caption.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Reasons for Unstable Training with the Soft-Actor-Critic Algorithm}{30}{subsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Experiments with the Proximal Policy Optimization Algorithm}{30}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Continuous Action Space}{30}{subsection.6.2.1}\protected@file@percent }
\citation{zeng2018learning}
\citation{zeng2019tossingbot}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces PPO performed extremely bad on the task without being able to learn the simplest set-up where the position of the object is constant during the whole training. In the first graphics there are some cases of reward 1 - there are so rare that they are most likely accidental.\relax }}{31}{figure.caption.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Discrete Action Space without Gripper Rotation}{31}{subsection.6.2.2}\protected@file@percent }
\newlabel{evaluation:multidiscrete_no_rotation}{{6.2.2}{31}{Discrete Action Space without Gripper Rotation}{subsection.6.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces 300k step training of PPO on random position of the object. After approximately 60k steps the agent managed to establish a good policy and cope with 100\% success rate on training cases. \relax }}{31}{figure.caption.22}\protected@file@percent }
\citation{tang2019discretizing}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces 500k step training of PPO on random position of the object. The evaluation of the model at the end showed 99.5\% success rate with 7\% variance. \relax }}{32}{figure.caption.23}\protected@file@percent }
\newlabel{exampletab}{{\caption@xref {exampletab}{ on input line 252}}{32}{Discrete Action Space without Gripper Rotation}{figure.caption.24}{}}
\newlabel{sub@exampletab}{{}{32}{Discrete Action Space without Gripper Rotation}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces The PPO algorithm in combination with the discrete action space showed high success rates for all four test scenarios. In the fourth scenario during evaluation some of the object's positions were new to the robot, which is why he did not manage to grasp them right away or in 2\% at all. Increasing the number of training steps might close this gap. \relax }}{32}{figure.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Discrete Action Space with Gripper Rotation}{32}{subsection.6.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces The target object has a random rotation from 0$^\circ $ to 90$^\circ $, however it is possible for the gripper to grasp it at any times if the correct gripper position is determined.\relax }}{33}{figure.caption.25}\protected@file@percent }
\newlabel{exampletab}{{\caption@xref {exampletab}{ on input line 289}}{33}{Discrete Action Space with Gripper Rotation}{figure.caption.26}{}}
\newlabel{sub@exampletab}{{}{33}{Discrete Action Space with Gripper Rotation}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces The PPO algorithm in combination with the discrete action space and gripper rotation showed high success rates for all four test scenarios. In the third scenario with 50 possible positions the success rate is lower and the variance is higher than in the fourth case, which is a not expected behavior as the third scenario is meant to be easier than the fourth one. Increasing the number of training steps might be helpful to get better results for the third case as the agent will experience more object positions and thus can better learn. \relax }}{33}{figure.caption.26}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Generalization Test}{34}{section.6.3}\protected@file@percent }
\newlabel{fig:gen_test_sac}{{6.12a}{35}{SAC with continuous action space\relax }{figure.caption.27}{}}
\newlabel{sub@fig:gen_test_sac}{{a}{35}{SAC with continuous action space\relax }{figure.caption.27}{}}
\newlabel{fig:gen_test_PPO_no_rotation}{{6.12b}{35}{PPO with discrete action space without rotation\relax }{figure.caption.27}{}}
\newlabel{sub@fig:gen_test_PPO_no_rotation}{{b}{35}{PPO with discrete action space without rotation\relax }{figure.caption.27}{}}
\newlabel{fig:gen_test_PPO_with_rotation}{{6.12c}{35}{PPO with discrete action space with rotation\relax }{figure.caption.27}{}}
\newlabel{sub@fig:gen_test_PPO_with_rotation}{{c}{35}{PPO with discrete action space with rotation\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces The diagrams show results of evaluation and generalization tests for three experiments: SAC with continuous action space, PPO with discrete action space without rotation, PPO with discrete action space with rotation. The x-axis represents the number of object positions that the agent was trained on, with "Random" being random object position during training. Green represents the success rate of the agent during the evaluation on the scenario that it was trained with, grey is the corresponding variance. Blue is the success rate at the generalization test, red is its variance. \relax }}{35}{figure.caption.27}\protected@file@percent }
\newlabel{fig:gen_test}{{6.12}{35}{The diagrams show results of evaluation and generalization tests for three experiments: SAC with continuous action space, PPO with discrete action space without rotation, PPO with discrete action space with rotation. The x-axis represents the number of object positions that the agent was trained on, with "Random" being random object position during training. Green represents the success rate of the agent during the evaluation on the scenario that it was trained with, grey is the corresponding variance. Blue is the success rate at the generalization test, red is its variance. \relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Simulation Inaccuracies}{35}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}The Angle Of The Gripper}{35}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Slipping Of The Object}{36}{subsection.6.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{37}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.0.1}Discussion}{37}{subsection.7.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.0.2}Future Work}{37}{subsection.7.0.2}\protected@file@percent }
\@setckpt{content/results}{
\setcounter{page}{39}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{7}
\setcounter{section}{0}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{caption@flags}{4}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{lstnumber}{8}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{2}
}
