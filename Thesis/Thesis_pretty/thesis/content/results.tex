
% The movement of the robot consists of several steps: the robot rotates the gripper to the angle $\alpha$, after that it goes to the target (x,y) coordinate. The z-coordinate of the gripper stays constant during the first two parts of the movement. After reaching the target positions: (x,y, $\alpha$), the robot executes a planar grasp: it goes down to almost reach the height of the table with the max gripper opening, then it closes the jaws and goes up. In the final position, when the gripper is above the table, the width between the jaws of the gripper is calculated. If it is not null, then the object was successfully grasped. \\
% In reality the simulation engine showed an unexpected behavior: when the gripper was down, closed the jaws and successfully grasped the object, the object did not stay through the whole way up, it slipped from the jaws. According to Mujoco documentation the type of numerical algorithm for solving convex optimization problems - solver - could be changed to reach the desirable behavior as well as some other parameters such as "$noslip_iterations$", "cone", "impratio". However tuning in those parameters did not help to solve the problem. The method that helped was to slightly close the gripper on the way up. \\
% The observation input was an RGB 81x81 image, the action space continuous: $x \in [1.1, 1.45]$,  $y \in [0.55, 0.95]$, $\alpha \in [0, 90]$.\\
% In the beginning the algorithm was tested for the simplified task: the test object was a black cube with constant position on the table, within the reach of the robot arm. \\
% The first algorithm tested was DDPG with 25 000 steps. The expected outcome was that the model will memorize the position of the cube and will always predict the correct grasping action. \\

\chapter{Evaluation}\label{chapter:evaluation}

In order to train and evaluate the algorithms several test case scenarios were designed. By starting from a simple scenario and increasing difficulty it could be determined at what stage and whether the algorithm is experiencing difficulties and its performance overall. \\
Three lists of random object positions within the limits of the workspace were randomly generated. The first list contained only one position, the second - 20, the third - 50, each list was used in different scenarios. During one iteration in each scenario the object could be located on one of the positions of the corresponding list. The first scenario is considered to be the easiest one, as the object is always located on the same spot. In the second scenario the object can be located on one of the 20 possible positions, in the third one - on one of the 50 possible positions.\\
In the fourth scenario the object's position is randomly determined for each iteration - which corresponds to a list with infinite object positions, making it the most difficult scenario. At the same time it is the most important case as it does not include any limitations for the object's position (apart from being located within the camera view on the workspace) and if the agent can perform well on this scenario, it should be able to cope with the first three easier ones. 



\section{Experiments with the Soft-Actor-Critic Algorithm}
Several experiments using the Soft-Actor-Critic(SAC) algorithm described in section \ref{section: SAC} in combination with continuous action space were conducted. The results were unstable: repeating the same experiment with same parameters delivered different results. In some cases the training was successful. However sometimes the network was showing good performance which dropped after some training steps, resulting in 0\% success rate at the end. Saving the model several times during training helped to retrieve the model that was giving a good performance. In some trainings the agent never achieved good performance. There might be several reasons for such unstable behavior, they will be discussed later on.  \\

For all four test scenarios the best agents are represented in the table below. The agents show high success rates and relatively low variance. These best agents were retrieved during successful training runs, however if the experiments will be repeated again, the results might be worse because of instability and inconsistency of the algorithm. 
\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\begin{table}[H]
			\begin{center}
				\footnotesize
				\begin{tabular}{|l|l|l|l|l|l|}
					\hline
					\bf Test case & \bf Success rate, \% & \bf Variance, \% \\ \hline
					1 object position  & 100 & 0 \\ \hline
					20 object positions  & 100 & 0 \\ \hline
					50  object positions  & 90 & 30 \\ \hline
					Random position& 97 & 17 \\ \hline
				\end{tabular}
			\end{center}
			\label{exampletab}
			% \caption{}
		\end{table}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.5\textwidth}
	\centering
	\includegraphics[width=1.0\textwidth]{graphics/results_sac}
	% \caption{}
	\end{subfigure}%
	\caption{Results of the evaluation of best agents for each test scenario. These agents show high success rate and low variance. However if the trainings for each experiment will be repeated, the resulting agents might be not as successful as the ones represented in the table.}
\end{figure} 

Sections \ref{evaluation:one_obj_SAC} and \ref{evaluation:20_obj_SAC} show statistics of several trainings with the SAC algorithm. Section \ref{evaluation:one_obj_SAC}  illustrates the unexpected behavior of success rate dropping during training and inconsistency of the training. Section \ref{evaluation:20_obj_SAC} disclosures more information about one of the successful runs.

\subsection{One  Constant Positions Of The Object}\label{evaluation:one_obj_SAC}
Among successful experiment runs, where the position of the object was learned even under 10k training steps, there were some that were not stable. As an example a training that lasted 30k steps. After 14k steps the agent was performing with 100\% sucessrate. However shortly after the performance dropped and never recovered. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{graphics/1_obj_pos_SAC/episode_reward}
	\caption{An example of unstable behavior of the algorithm based on the reward value: after 14k training steps the agent learned correctly where the object is located. Then the performance dropped to 0 and did not recover after that.}
\end{figure}

\subsection{20 Constant Positions Of The Object}\label{evaluation:20_obj_SAC}
A list of 20 $(x,y)$ coordinates in the range of robot's action space were randomly generated. During each episode the object is located on the table with its $(x,y)$ coordinates being one random position from the list. The goal of the experiment was to determine whether the network is able to learn how to grasp the object. \\

The training statistics is: \\ 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{graphics/20_obj_pos_150k/episode_reward}
	\caption{After about 45k training steps the success rate was almost always 1. The reason it dropped in some episodes might be due to inaccuracy in actions or some object positions occurred for the first time - the agent never saw them before which led to bad performance, they were learned after that.}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/ent_coef}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
		\label{fig:sub-first}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/ent_coef_loss}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	~
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/entropy}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/learning_rate}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/policy_loss}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/qf1_loss}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/qf2_loss}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/value_loss}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}
	\caption{Statistics of the training: object's position is one of 20 possible ones, learn to grasp the object in 150k steps. At the end of the training the entropy is becoming constant which means that the algorithm is sure which action to take. The entropy coefficient is going down as well because the agent does not need to do much exploration anymore. Policy loss is going to zero, as well as the losses from both Q-networks and the target value network, which means the weights of the network are adjusted in an optimal way to achieve success. }
\end{figure}

The evaluation consisted of 300 episodes, 100\% success rate - the task was completed successfully. \\
The 150000 steps of the training resulted in 43998 episodes. As max\_episode\_steps value was set to 50, in the beginning the majority of episodes lasted 50 steps, however at the end of the training the value shrank to 1, occasionally going up. \\

\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/episode_lengths_first10k}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/20_obj_pos_150k/episode_lengths_last10k}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}
    \caption{The first image shows the statistics of the length of first 10k episodes of the training, the second one - of the last 10k. It is noticeable, that in the second case the majority of episodes were short because the agent has already learned most of positions of the objects. In case of inaccuracy of actions or incorrect assumptions about the location of the object the  episode lasted longer - up to 50 steps.}
\end{figure} 

\begin{comment}
\subsection{50 Constant Positions Of The Object}\label{evaluation:50_obj_SAC}
A list of 50 (x,y) coordinates in the range of robot's action space were randomly generated. During each episode the object is located on the table with its (x,y) coordinates one random position from the list. \\
Same experiments showed different results. In some cases the agent did not learn at all. Below there are two examples of training in which the agent reached relatively good performance. The first one is a successful learned model which achieved success rate 97\%. The second one showed 90\% success rate during evaluation with standard deviation approximately 30\% - the agent was not always sure which action to take. \\  

\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/50_obj_pos_SAC/episode_reward_successful}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/50_obj_pos_SAC/episode_reward_ok}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}
	\caption{The same experiment that differed only in number of training steps. The first image shows that in the first experiment the agent learned positions successfully after 80k steps, in the second experiment the agent did not manage to succeed even after 250k steps of training. The evaluation proved it: the first agent always graspes the object, the second one only in 90\% of cases with a high uncertainty of 30\%.}
\end{figure} 

The training statistics show, that the entropy factor became constant in the first experiment, in the second one it variated: 

\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/50_obj_pos_SAC/entropy_successful}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/50_obj_pos_SAC/entropy_ok}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}
	\caption{Entropy loss of the successful agent became constant after about 80k steps - when the agent learned the task. In the second experiment the agent was not sure which action to take.}
\end{figure} 

An unexpected behavior showed the statistics about the value loss. In the case of the successful agent the value loss was extremely high (from 1e+4 to 5e+5), going up and then down during training, which did not effect the agent's success rate, it stayed 100\%. The not so successful agent's value loss was ranging from 0 to 1.
\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/50_obj_pos_SAC/value_loss_successful}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/50_obj_pos_SAC/value_loss_ok}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}
	\caption{Value loss statistics of two experiment.}
\end{figure} 
\end{comment}

\begin{comment}
\subsection{Random Position Of The Object}
In previous experiments the agent was expected to learn finite number of positions, which should have been an easy task because he could just learn them by heart. In case of a random position of the object, it is impossible to learn all combinations as there are endless. \\
After 400k steps of training during the evaluation the agent performed with 97\% success rate and 17\% variance. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{graphics/random_SAC/episode_reward}
	\caption{400k step training of SAC on random position of the object}
\end{figure}
\end{comment}

\subsection{Reasons for Unstable Training with the Soft-Actor-Critic Algorithm}
Soft-Actor-Critic is a state-of-the-art reinforcement learning algorithm, it showed impressive results on such gym tasks as Humanoid-v2 and Ant-v2 \cite{haarnoja2018soft}. The current grasping task is much more simple with a smaller action space, which is why the instable training results are very unlikely to be caused by the algorithm. Another reason could be the instablity of the environment. However, training with PPO was stable: repeating the experiments delivered same expected results, which means the task can be trained in the created environment. Another assumption about the cause of unstable behavior is the implementation of SAC by stable\_baselines. The actual reason should be discovered in future work. 


\section{Experiments with the Proximal Policy Optimization Algorithm}

\subsection{Continuous Action Space}
The algorithm performed very poorly in the \hyperref[section:continuous_action_space]{continuous}  action space. It failed to learn even the simplest task where the object's position does not change. 
\begin{figure}[H]
	\centering
		\centering
		\includegraphics[width=0.5\textwidth]{graphics/continuous_PPO/episode_reward}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\caption{PPO performed extremely bad on the task without being able to learn the simplest set-up where the position of the object is constant during the whole training. In the first graphics there are some cases of reward 1 - there are so rare that they are most likely accidental.}
\end{figure} 

\subsection{Discrete Action Space without Gripper Rotation}\label{evaluation:multidiscrete_no_rotation}
Following the idea of Zeng et al. in \cite{zeng2018learning}, \cite{zeng2019tossingbot}, the action space was \hyperref[section:multidiscrete_no_rotation]{discretized} to consist of a 50 $\times$ 50 grid. The action $(x,y)$ would mean the gripper would execute a planar grasp in the middle of the cell $(x,y)$. \\
The results of the experiments with 1, 20 and 50 constant positions of the object were successful: during evaluation for all these cases the agent performed with almost 100\% success rate. \\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{graphics/50_obj_pos_PPO/episode_reward}
	\caption{300k step training of PPO on random position of the object. After approximately 60k steps the agent managed to establish a good policy and cope with 100\% success rate on training cases. }
\end{figure}
For random object position after 300k training steps the success rate was 98\% with 11\% variance. The model created in another training with 500k steps showed 99.5\% success rate with 7\% variance during 1000 evaluation steps. \\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{graphics/random_PPO_500k/episode_reward}
	\caption{500k step training of PPO on random position of the object. The evaluation of the model at the end showed 99.5\% success rate with 7\% variance.  }
\end{figure}

For all four test scenarios the best agents are represented in the table below. In the first three scenarios the agent coped perfectly with the task: 100\% success rate with 0 \% variance. The success rate in the forth scenario is 99.5\% with higher variance. It might be due to the fact that during evaluation some positions of the object were never seen by the robot in the training, which is why it was not able to grasp it or it took several attempts to grasp the object, as opposed to the first three scenarios, where the object was grasp from the first attempt. 
\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\begin{table}[H]
			\begin{center}
				\footnotesize
				\begin{tabular}{|l|l|l|l|l|l|}
					\hline
					\bf Test case & \bf Success rate, \% & \bf Variance, \% \\ \hline
					1 object position  & 100 & 0 \\ \hline
					20 object positions  & 100 & 0 \\ \hline
					50  object positions  & 100 & 0 \\ \hline
					Random position & 99.5 & 7 \\ \hline
				\end{tabular}
			\end{center}
			\label{exampletab}
			% \caption{}
		\end{table}
\end{subfigure}%
~	
\begin{subfigure}[h]{0.5\textwidth}
	\centering
	\includegraphics[width=1.0\textwidth]{graphics/results_ppo_without_rotation}
	% \caption{}
\end{subfigure}%
\caption{The PPO algorithm in combination with the discrete action space showed high success rates for all four test scenarios. In the fourth scenario during evaluation some of the object's positions were new to the robot, which is why he did not manage to grasp them right away or in 0.5\% at all. Increasing the number of training steps might close this gap. }
\end{figure} 
The idea of discretizing the action space was formulated in \cite{tang2019discretizing}. Although the idea is simple, it can drastically improve the performance of baseline on-policy algorithms.\\

\subsection{Discrete Action Space with Gripper Rotation}
In this scenario the \hyperref[section:multidiscrete_with_rotation]{action} space is discrete including gripper rotation. The test object has a random rotation from 0$^\circ$ to 90$^\circ$, however it is possible for the gripper to grasp it at any times if the correct gripper position is determined. \\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{graphics/ppo_with_rotation}
	\caption{The target object has a random rotation from 0$^\circ$ to 90$^\circ$, however it is possible for the gripper to grasp it at any times if the correct gripper position is determined.}
\end{figure}
For all four test scenarios the best agents are represented in the table below. The results are not as good as in the \hyperref[evaluation:multidiscrete_no_rotation]{previous} case, where there was no rotation of the object and the gripper. The reason might be due to the fact that the task was more complicated with the bigger action space. 
\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\begin{table}[H]
			\begin{center}
				\footnotesize
				\begin{tabular}{|l|l|l|l|l|l|}
					\hline
					\bf Test case & \bf Success rate, \% & \bf Variance, \% \\ \hline
					1 object position  & 100 & 0 \\ \hline
					20 object positions  & 98 & 14 \\ \hline
					50  object positions  & 90 & 30 \\ \hline
					Random position & 98 & 15 \\ \hline
				\end{tabular}
			\end{center}
			\label{exampletab}
			% \caption{}
		\end{table}
	\end{subfigure}%
      ~	
		\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/results_ppo_with_rotation}
		% \caption{}
		\end{subfigure}%
	\caption{The PPO algorithm in combination with the discrete action space and gripper rotation showed high success rates for all four test scenarios. In the third scenario with 50 possible positions the success rate is lower and the variance is higher than in the fourth case, which is a not expected behavior as the third scenario is meant to be easier than the fourth one. Increasing the number of training steps might be helpful to get better results for the third case as the agent will experience more object positions and thus can better learn. }
\end{figure} 

During evaluation it was noticeable that the gripper made several attempts before successfully grasping the object. Often during these attempts the object was slightly shifted and rotated, accidentally resulting in more convenient for the gripper positions to be grasped. This kind of approach might be effective in simulation, however it is not desired in the real-world setup - the gripper's actions should be precise and be aimed at grasping the object at first attempt. 

\section{Generalization Test}
For each experiment four different training scenarios were conducted: one, 20, 50 and random object positions. For the first three scenarios the agents were evaluated using the same list of object positions that they were trained on. This brings up the following question: how well would these agents perform on the general case with the object position being randomly assigned on the working space? \\
The generalization test was conducted. The results showed, that the agents that were trained on specific object positions were not able to show good performance on the general case where the object position was random. \\
The results of the generalization test together with evaluation results are shown in figure \ref{fig:gen_test}. The expected behavior is shown in figure \ref{fig:gen_test_PPO_no_rotation}: with the rising number of object positions seen in the training the success rate for the general case rises and the variance becomes lower. \\
In figure \ref{fig:gen_test_sac} this idea is violated by the case with 50 possible positions of the object. This can be explained by the the fact that the trained agent for this case performed worse during its own evaluation: lower success rate and higher variance as in the case with 20 objects. Which suggests that the agent was not well trained overall, which is why he performed very poorly at the generalization test. \\
In figure \ref{fig:gen_test_PPO_with_rotation} the agents trained for 20 and 50 object positions showed a high success rate for the general case - over 75\%. However the variance for both  cases is over 40\%, which means the agents were often not entirely sure which action to take and after several attempts succeeded to chose the right one. Such high variance signifies that these trained agents cannot be used for the general case as they are too uncertain in their actions. \\

\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/gen_test_SAC}
		\caption{SAC with continuous action space}\label{fig:gen_test_sac}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/gen_test_PPO_no_rotation}
		\caption{PPO with discrete action space without rotation}\label{fig:gen_test_PPO_no_rotation}
	\end{subfigure}
	~	
	\begin{subfigure}[h]{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/gen_test_PPO_with_rotation}
		\caption{PPO with discrete action space with rotation}\label{fig:gen_test_PPO_with_rotation}
	\end{subfigure}
	~	
	\begin{subfigure}[h]{0.4\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/legend}
		% \caption{PPO with discrete action space with rotation}
	\end{subfigure}
	\caption{The diagrams show results of evaluation and generalization tests for three experiments: SAC with continuous action space, PPO with discrete action space without rotation, PPO with discrete action space with rotation. The x-axis represents the number of object positions that the agent was trained on, with "Random" being random object position during training. Green represents the success rate of the agent during the evaluation on the scenario that it was trained with, grey is the corresponding variance. Blue is the success rate at the generalization test, red is its  variance. }\label{fig:gen_test}
\end{figure} 

\section{Simulation Inaccuracies}
During the setup of simulation some unexpected behaviors occurred that had a significant influence on the results of the experiment. Until these difficulties were eliminated, it was impossible to train a successful agent. 
\subsection{The Angle Of The Gripper}
The action space consists of target $(x,y)$ coordinates of the point that the gripper has to achieve before going down. The step() function computes the difference between target and current positions, the difference is then applied to the simulation using inverse kinematics. The inverse kinematics is calculated by solving the convex optimization problem. The optimization problem is defined combining description of bodies that are used in the simulation and physical constraints such as forces, friction, etc.\\
One of the challenges that was faced during the set-up is that there can be many solutions to the problem of gripper achieving the target $(x,y)$ position. In some cases the gripper developed an angle in different plains, successfully reaching target position but the angle made it impossible to compute a successful grasp. 
\begin{figure*}[h!]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/simulation_problems/angle_1}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
	~	
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{graphics/simulation_problems/angle_2}
		% \caption{Model-based RL Quelle: s}\label{fig:RL}
	\end{subfigure}%
\end{figure*}

Setting the robot to the starting position for every step helped to correct that inaccuracy for the current task. However, this approach is time-consuming and might be not applicable for similar problems in different set-ups. The possible better solution to this problem might be to set additional constraints for the position of the joints of the robot that would effect the solution of the inverse kinematics problem. \\

\subsection{Slipping Of The Object}
The first version of grasping consisted of completely closing the gripper and going up with an object. However, the object did not stay in the gripper - it slipped down. This behavior was not expected as physical forces such as friction were set to default values. The solution to the problem was to keep closing the gripper while going up - it helps to prevent the object from slipping. \\


 \chapter{Conclusion}
In this thesis the system for robotic vision-based grasping with reinforcement learning in simulation is presented. Two reinforcement learning algorithms were tested in combination with different representations of action spaces. The reward for taken action was sparse. Discretizing action space can be a powerful tool that helps to achieve high success rate of a task. The generalization tests showed that with the increasing amount of randomness during experiments, the trained agents learn to perform better on the general case that does not include limitations.\\
Deep reinforcement learning helps to combine perception and control. The DRL approach can be applied to develop end-to-end self-supervised grasping systems that are data-driven and require little or no human involvement during training. \\
Training the robot in simulation is helpful for fast collecting large quantities of data with little cost. 

\section{Discussion}
Two reinforcement learning algorithms PPO and SAC were applied. The SAC algorithm in combination with continuous action space showed instable results, which might be due to a bug in the implementation from the stable baselines library. PPO for the case without rotation of the gripper in combination with discrete action space showed good consistent results, with success rate over 99\% for all test scenarios. PPO applied to continuous action space showed poor results not being able to solve even the simplest task where the object is always located on the same spot. PPO in combination with discrete action space including rotation of the gripper showed stable behavior with success rate over 90\% for all test scenarios. However, in this experiment during evaluation the gripper often needed several attempts to grasp the object, slightly adjusting the position of the object during each attempt in order to make it more convenient to grasp, which is not a desirable behavior for the real-world setup. \\
Additionally, the generalization test was conducted. The results showed that the agents trained for a specific case of predefined finite small number of positions do not perform well on the general case where the object position is random. The more randomness the agent experiences during training, the better its performance is on the general case. 
 
% It is especially important for complex data-driven systems as without sufficient data the 
 \section{Future Work}
The conducted approach included a set of limitations which suggest directions for future work. First of all, the robot should be able to grasp target objects of different shapes, sizes, forms and colors. What is more, the scenario of cluttered environment where there are multiple objects on the workspace partially covering each other should be researched. For more general problems planar grasps might not be sufficient to grasp the object, so pitch- and roll-angles of the gripper should be included in the action space of the robot. Other reinforcement leaning algorithms should be tried out as they might be more effective for the grasping problem. \\
The ultimate goal is to create a real-world system for robotic grasping. In future researches the transition from simulation to the real world environment must be analyzed in order to effectively transfer the agent pretrained in simulation, so that either the reality gap problem is eliminated during training in simulation or as little as possible additional real-world training is required. 



