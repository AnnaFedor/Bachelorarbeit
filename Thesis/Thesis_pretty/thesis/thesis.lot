\babel@toc {english}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Hyperparameters for for the applied SAC algorithm.\relax }}{23}{table.caption.13}% 
\contentsline {table}{\numberline {4.2}{\ignorespaces Hyperparameters for for the applied PPO algorithm.\relax }}{23}{table.caption.14}% 
\contentsline {table}{\numberline {4.3}{\ignorespaces Hyperparameters for for the applied PPO algorithm.\relax }}{24}{table.caption.15}% 
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces Results of the evaluation of best agents for each test scenario. These agents show high success rate and low variance. However if the trainings for each experiment will be repeated, the resulting agents might be not as successful as the ones represented in the table.\relax }}{26}{table.caption.16}% 
\contentsline {table}{\numberline {5.2}{\ignorespaces The PPO algorithm in combination with the discrete action space showed high success rates for all four test scenarios. In the fourth scenario during evaluation some of the object's positions were new to the robot, which is why he did not manage to grasp them right away or in 2\% at all. Increasing the number of training steps might close this gap. \relax }}{30}{table.caption.24}% 
\contentsline {table}{\numberline {5.3}{\ignorespaces The PPO algorithm in combination with the discrete action space and gripper rotation showed high success rates for all four test scenarios. In the third scenario with 50 possible positions the success rate is lower and the variance is higher than in the fourth case, which is a not expected behavior as the third scenario is meant to be easier than the fourth one. Increasing the number of training steps might be helpful to get better results for the third case as the agent will experience more object positions and thus can better learn. \relax }}{31}{table.caption.26}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
