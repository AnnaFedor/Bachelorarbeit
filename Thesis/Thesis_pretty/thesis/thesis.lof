\babel@toc {ngerman}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Outtake from "Computing 3-D Optimal Form-Closure Grasps" \cite {ding2000computing} of Ding et al.\relax }}{5}{figure.caption.3}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Classification of different aspects that influence the problem of the grasping problem according to \cite {bohg2013data} of Bohg et al.\relax }}{6}{figure.caption.4}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Reinforcement learning algorithms taxonomy Quelle: Spinning up Ich werde ein \IeC {\"a}hnliches Bild machen, aber nur mit Algorithmen, die ich benutzt habe.\relax }}{12}{figure.caption.6}% 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Model-based RL Quelle: s\relax }}{13}{figure.caption.7}% 
\contentsline {figure}{\numberline {2.5}{\ignorespaces TODO: make my own scheme\relax }}{14}{figure.caption.8}% 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Custom environment example for using stable baselines\relax }}{17}{figure.caption.9}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Custom environment example for using stable baselines\relax }}{20}{figure.caption.10}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Observation from the camera\relax }}{21}{figure.caption.11}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.2}{\ignorespaces An example of unstable behavior of the algorithm: after 14k training steps the agent learned correctly where the object is located. Then the performance dropped to 0 and did not recover after that.\relax }}{24}{figure.caption.17}% 
\contentsline {figure}{\numberline {5.3}{\ignorespaces After about 45k training steps the success rate was almost always 1. The reason it dropped in some episodes might be due to inaccuracy in actions or some object positions occurred for the first time - the agent never saw them before which lead to bad performance, they were learned after that\relax }}{25}{figure.caption.19}% 
\contentsline {figure}{\numberline {5.4}{\ignorespaces Statistics of the training: object's position is one of 20 possible ones, learn to grasp the object in 150k steps. At the end of the training the entropy is becoming constant which means that the algorithm is sure which action to take. The entropy coefficient is going down as well because the agent does not need to do much exploration anymore. Policy loss is going to zero, as well as the losses from both Q-networks and the target value network, which means the weights of the network are adjusted in an optimal way to achieve success. \relax }}{25}{figure.caption.20}% 
\contentsline {figure}{\numberline {5.5}{\ignorespaces The first image shows the statistics of the length of first 10k episodes of the training, the second one - of the last 10k. It is noticeable, that in the second case tha majority of episodes were short because the agent has already learned most of positions of the objects. In case of inaccuracy of actions or incorrect assumptions about the location of the object the episode lasted longer - up to 50 steps\relax }}{26}{figure.caption.21}% 
\contentsline {figure}{\numberline {5.6}{\ignorespaces The same experiment that differed only in number of training steps. The first image shows that in the first experiment the agent learned positions successfully after 80k steps, in the second experiment the agent did not manage to succeed even after 250k steps of training. The evaluation proved it: the first agent always graspes the object, the second one only in 90\% of cases with a high uncertainty of 30\%.\relax }}{26}{figure.caption.23}% 
\contentsline {figure}{\numberline {5.7}{\ignorespaces Entropy loss of the successful agent became constant after about 80k steps - when the agent learned the task. In the second experiment the agent was not sure which action to take.\relax }}{27}{figure.caption.24}% 
\contentsline {figure}{\numberline {5.8}{\ignorespaces Value loss statistics of two experiment.\relax }}{27}{figure.caption.25}% 
\contentsline {figure}{\numberline {5.9}{\ignorespaces 400k step training of SAC on random position of the object\relax }}{28}{figure.caption.27}% 
\contentsline {figure}{\numberline {5.10}{\ignorespaces PPO performed extremely bad on the task without being able to learn the simplest set-up where the position of the object is constant during the whole training. In the first graphics there are some cases of reward 1 - there are so rare that they are most likely accidental.\relax }}{28}{figure.caption.31}% 
\contentsline {figure}{\numberline {5.11}{\ignorespaces 300k step training of PPO on random position of the object\relax }}{29}{figure.caption.33}% 
\contentsline {figure}{\numberline {5.12}{\ignorespaces 500k step training of PPO on random position of the object. The evaluation of the model at the end showed 99.5\% success rate with 7\% variance. \relax }}{29}{figure.caption.34}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
