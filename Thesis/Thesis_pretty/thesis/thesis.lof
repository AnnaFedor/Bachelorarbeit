\babel@toc {english}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Outtake from "Computing 3-D Optimal Form-Closure Grasps" \cite {ding2000computing} by Ding et al. The image shows the position of the friction cone at a grasping point on the object's surface. In order for fingers not to slip while executing the grasp, the finger force must lie in the friction cone.\relax }}{7}{figure.caption.5}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Classification of different aspects that influence the grasping problem according to \cite {bohg2013data} of Bohg et al. The most important aspect is the prior knowledge of the object.\relax }}{8}{figure.caption.6}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Description of the SAC algorithm from the original paper \cite {haarnoja2018soft}. First the agent collects some experience which is saved in the replay buffer D. After that all five networks are updated using information from the replay buffer.\relax }}{16}{figure.caption.7}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The goal of the DRL approach is to develop a policy that decides which action at which step the agent should take. In order to do that the agent interacts with the environment by observing environment's state, taking actions and getting rewards for them. This way the agent determines through trial-and-error the correct behavior. Source of the image: \cite {mao2016resource}\relax }}{19}{figure.caption.8}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Custom environment example for using stable baselines. The CustomEnv inherits from gym.Env and implements methods {\_\_init\_\_()}, step(), reset(), render(), close() which is a requirement to be able to train using one of the RL algorithm's implementation from stable baselines.\relax }}{22}{figure.caption.12}% 
\contentsline {figure}{\numberline {4.2}{\ignorespaces The camera on the side of the table takes an RGB-image of the part of the table where the object can be located. The second image is an example of the observation that is used as state representation and input to the neural network\relax }}{23}{figure.caption.13}% 
\contentsline {figure}{\numberline {4.3}{\ignorespaces In the first image the gripper's jaws are fully closed after the grasped attempt - the object was not grasped. In the second image the attempt was successful, the object is between the jaws preventing them from closing, so the distance between the jaws is slightly greater or equals the width of the object.\relax }}{24}{figure.caption.14}% 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Continuous Action Space: the action $(x, y)$ is target cartesian coordinates of the gripper.\relax }}{24}{figure.caption.15}% 
\contentsline {figure}{\numberline {4.5}{\ignorespaces Discrete Action Space. The workspace is represented as $50 \times 50$ grid, the action $(x, y)$ means going to the middle of the cell number $(x, y)$ and completing the planar grasp.\relax }}{25}{figure.caption.16}% 
\contentsline {figure}{\numberline {4.6}{\ignorespaces Discrete Action Space including rotation. The workspace is represented as $50 \times 50$ grid, the action $(x, y, \alpha $) means going to the middle of the cell number $(x, y)$, rotating the gripper value in degrees that corresponds to $\alpha $ and completing the planar grasp.\relax }}{26}{figure.caption.17}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Results of the evaluation of best agents for each test scenario. These agents show high success rate and low variance. However if the trainings for each experiment will be repeated, the resulting agents might be not as successful as the ones represented in the table.\relax }}{30}{figure.caption.21}% 
\contentsline {figure}{\numberline {5.2}{\ignorespaces An example of unstable behavior of the algorithm based on the reward value: after 14k training steps the agent learned correctly where the object is located. Then the performance dropped to 0 and did not recover after that.\relax }}{30}{figure.caption.22}% 
\contentsline {figure}{\numberline {5.3}{\ignorespaces After about 45k training steps the success rate was almost always 1. The reason it dropped in some episodes might be due to inaccuracy in actions or some object positions occurred for the first time - the agent never saw them before which led to bad performance, they were learned after that.\relax }}{31}{figure.caption.23}% 
\contentsline {figure}{\numberline {5.4}{\ignorespaces Statistics of the training: object's position is one of 20 possible ones, learn to grasp the object in 150k steps. At the end of the training the entropy is becoming constant which means that the algorithm is sure which action to take. The entropy coefficient is going down as well because the agent does not need to do much exploration anymore. Policy loss is going to zero, as well as the losses from both Q-networks and the target value network, which means the weights of the network are adjusted in an optimal way to achieve success. \relax }}{31}{figure.caption.24}% 
\contentsline {figure}{\numberline {5.5}{\ignorespaces The first image shows the statistics of the length of first 10k episodes of the training, the second one - of the last 10k. It is noticeable, that in the second case the majority of episodes were short because the agent has already learned most of positions of the objects. In case of inaccuracy of actions or incorrect assumptions about the location of the object the episode lasted longer - up to 50 steps.\relax }}{32}{figure.caption.25}% 
\contentsline {figure}{\numberline {5.6}{\ignorespaces PPO performed extremely bad on the task without being able to learn the simplest set-up where the position of the object is constant during the whole training. In the first graphics there are some cases of reward 1 - there are so rare that they are most likely accidental.\relax }}{33}{figure.caption.26}% 
\contentsline {figure}{\numberline {5.7}{\ignorespaces 300k step training of PPO on random position of the object. After approximately 60k steps the agent managed to establish a good policy and cope with 100\% success rate on training cases. \relax }}{33}{figure.caption.27}% 
\contentsline {figure}{\numberline {5.8}{\ignorespaces 500k step training of PPO on random position of the object. The evaluation of the model at the end showed 99.5\% success rate with 7\% variance. \relax }}{34}{figure.caption.28}% 
\contentsline {figure}{\numberline {5.9}{\ignorespaces The PPO algorithm in combination with the discrete action space showed high success rates for all four test scenarios. In the fourth scenario during evaluation some of the object's positions were new to the robot, which is why he did not manage to grasp them right away or in 0.5\% at all. Increasing the number of training steps might close this gap. \relax }}{34}{figure.caption.29}% 
\contentsline {figure}{\numberline {5.10}{\ignorespaces The target object has a random rotation from 0$^\circ $ to 90$^\circ $, however it is possible for the gripper to grasp it at any times if the correct gripper position is determined.\relax }}{35}{figure.caption.30}% 
\contentsline {figure}{\numberline {5.11}{\ignorespaces The PPO algorithm in combination with the discrete action space and gripper rotation showed high success rates for all four test scenarios. In the third scenario with 50 possible positions the success rate is lower and the variance is higher than in the fourth case, which is a not expected behavior as the third scenario is meant to be easier than the fourth one. Increasing the number of training steps might be helpful to get better results for the third case as the agent will experience more object positions and thus can better learn. \relax }}{35}{figure.caption.31}% 
\contentsline {figure}{\numberline {5.12}{\ignorespaces The diagrams show results of evaluation and generalization tests for three experiments: SAC with continuous action space, PPO with discrete action space without rotation, PPO with discrete action space with rotation. The x-axis represents the number of object positions that the agent was trained on, with "Random" being random object position during training. Green represents the success rate of the agent during the evaluation on the scenario that it was trained with, grey is the corresponding variance. Blue is the success rate at the generalization test, red is its variance. \relax }}{37}{figure.caption.32}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
