\babel@toc {english}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Outtake from "Computing 3-D Optimal Form-Closure Grasps" \cite {ding2000computing} of Ding et al.\relax }}{3}{figure.caption.3}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Classification of different aspects that influence the problem of the grasping problem according to \cite {bohg2013data} of Bohg et al.\relax }}{4}{figure.caption.4}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Reinforcement learning algorithms taxonomy Quelle: Spinning up Ich werde ein \IeC {\"a}hnliches Bild machen, aber nur mit Algorithmen, die ich benutzt habe.\relax }}{11}{figure.caption.6}% 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Model-based RL Quelle: s\relax }}{12}{figure.caption.7}% 
\contentsline {figure}{\numberline {2.5}{\ignorespaces TODO: make my own scheme\relax }}{13}{figure.caption.8}% 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Custom environment example for using stable baselines\relax }}{16}{figure.caption.9}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The goal of the Deep Reinforcement learning approach is to develop a policy that decides which action at which step the agent should take. In order to do that the agent interacts with the environment by observing environment's state, taking actions and getting rewards for them. This way the agent determines through trial-and-error the correct behavior. \relax }}{20}{figure.caption.10}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Custom environment example for using stable baselines. The CustomEnv inherits from gym.Env and implements methods {\_\_init\_\_()}, step(), reset(), render(), close() which is a requirement to be able to train using one of the reinforcement leaning algorithm's implementation from stable baselines.\relax }}{22}{figure.caption.11}% 
\contentsline {figure}{\numberline {4.2}{\ignorespaces The camera on the side of the table takes an RGB-image of the part of the table where the object can be located. The second image is an example of the observation that is used as state representation and input to the neural network\relax }}{23}{figure.caption.12}% 
\contentsline {figure}{\numberline {4.3}{\ignorespaces In the first image the gripper's jaws are fully closed after the grasped attempt - the object was not grasped. In the second image the attempt was successful, the object is between the jaws preventing them from closing, so the distance between the jaws is slightly greater or equals the width of the object.\relax }}{24}{figure.caption.13}% 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Continuous Action Space: the action (x, y) is target cartesian coordinates of the gripper.\relax }}{24}{figure.caption.14}% 
\contentsline {figure}{\numberline {4.5}{\ignorespaces Multidiscrete Action Space. The workspace is represented as 50*50 grid, the action (x, y) means going to the middle of the cell number (x, y) and completing the planar grasp.\relax }}{25}{figure.caption.15}% 
\contentsline {figure}{\numberline {4.6}{\ignorespaces Multidiscrete Action Space including rotation. The workspace is represented as 50*50 grid, the action (x, y, $\alpha $) means going to the middle of the cell number (x, y), rotating the gripper value in degrees that corresponds to $\alpha $ and completing the planar grasp.\relax }}{26}{figure.caption.16}% 
\contentsline {figure}{\numberline {4.7}{\ignorespaces The characteristics of the network. The network consists of three convolutional layers, followed by one fully connected layer. After each layer the ReLU function is applied.\relax }}{28}{figure.caption.19}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.2}{\ignorespaces An example of unstable behavior of the algorithm: after 14k training steps the agent learned correctly where the object is located. Then the performance dropped to 0 and did not recover after that.\relax }}{30}{figure.caption.24}% 
\contentsline {figure}{\numberline {5.3}{\ignorespaces After about 45k training steps the success rate was almost always 1. The reason it dropped in some episodes might be due to inaccuracy in actions or some object positions occurred for the first time - the agent never saw them before which lead to bad performance, they were learned after that\relax }}{31}{figure.caption.26}% 
\contentsline {figure}{\numberline {5.4}{\ignorespaces Statistics of the training: object's position is one of 20 possible ones, learn to grasp the object in 150k steps. At the end of the training the entropy is becoming constant which means that the algorithm is sure which action to take. The entropy coefficient is going down as well because the agent does not need to do much exploration anymore. Policy loss is going to zero, as well as the losses from both Q-networks and the target value network, which means the weights of the network are adjusted in an optimal way to achieve success. \relax }}{31}{figure.caption.27}% 
\contentsline {figure}{\numberline {5.5}{\ignorespaces The first image shows the statistics of the length of first 10k episodes of the training, the second one - of the last 10k. It is noticeable, that in the second case tha majority of episodes were short because the agent has already learned most of positions of the objects. In case of inaccuracy of actions or incorrect assumptions about the location of the object the episode lasted longer - up to 50 steps\relax }}{32}{figure.caption.28}% 
\contentsline {figure}{\numberline {5.6}{\ignorespaces The same experiment that differed only in number of training steps. The first image shows that in the first experiment the agent learned positions successfully after 80k steps, in the second experiment the agent did not manage to succeed even after 250k steps of training. The evaluation proved it: the first agent always graspes the object, the second one only in 90\% of cases with a high uncertainty of 30\%.\relax }}{32}{figure.caption.30}% 
\contentsline {figure}{\numberline {5.7}{\ignorespaces Entropy loss of the successful agent became constant after about 80k steps - when the agent learned the task. In the second experiment the agent was not sure which action to take.\relax }}{33}{figure.caption.31}% 
\contentsline {figure}{\numberline {5.8}{\ignorespaces Value loss statistics of two experiment.\relax }}{33}{figure.caption.32}% 
\contentsline {figure}{\numberline {5.9}{\ignorespaces 400k step training of SAC on random position of the object\relax }}{34}{figure.caption.34}% 
\contentsline {figure}{\numberline {5.10}{\ignorespaces PPO performed extremely bad on the task without being able to learn the simplest set-up where the position of the object is constant during the whole training. In the first graphics there are some cases of reward 1 - there are so rare that they are most likely accidental.\relax }}{34}{figure.caption.37}% 
\contentsline {figure}{\numberline {5.11}{\ignorespaces 300k step training of PPO on random position of the object\relax }}{35}{figure.caption.39}% 
\contentsline {figure}{\numberline {5.12}{\ignorespaces 500k step training of PPO on random position of the object. The evaluation of the model at the end showed 99.5\% success rate with 7\% variance. \relax }}{35}{figure.caption.40}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
